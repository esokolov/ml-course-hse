\documentclass[12pt,fleqn]{article}
\usepackage{vkCourseML}
\hypersetup{unicode=true}
%\usepackage[a4paper]{geometry}
\usepackage[hyphenbreaks]{breakurl}

\interfootnotelinepenalty=10000

\begin{document}
\title{Лекция 6\\Линейная классификация}
\author{Е.\,А.\,Соколов\\ФКН ВШЭ}
\maketitle

Ранее мы изучили общий подход к обучению линейных классификаторов, основанный на минимизации
верхней оценки:
\[
    \frac{1}{\ell}
    \sum_{i = 1}^{\ell}
        L(y_i, \langle w, x_i \rangle)
    \to
    \min_{w}
\]
При этом мы привели примеры нескольких верхних оценок~$\tilde L(M)$,
среди которых были
\begin{itemize}
    \item $L(y, z) = \log(1 + \exp(-yz))$~--- логистическая функция потерь;
    \item $L(y, z) = (1 - yz)_+$~--- кусочно-линейная функция потерь~(hinge loss).
\end{itemize}
В этой лекции мы выясним, откуда взялись эти функции потерь, и изучим некоторые их важные свойства.

\section{Логистическая регрессия}
\subsection{Оценивание вероятностей}
Метод обучения, который получается при использовании логистической функции потерь,
называется логистической регрессией.
Основным его свойством является тот факт, что он корректно оценивает вероятность
принадлежности объекта к каждому из классов.

Пусть в каждой точке пространства объектов~$x \in \XX$ задана вероятность~$p(y = +1 \cond x)$
того, что объект~$x$ будет принадлежать классу~$+1$.
Это означает, что мы допускаем наличие в выборке нескольких объектов
с одинаковым признаковым описанием, но с разными значениями целевой переменной;
причём если устремить количество объекта~$x$ в выборке к бесконечности,
то доля положительных объектов среди них будет стремиться к~$p(y = +1 \cond x)$.

Примером может служить задача предсказания кликов по рекламным баннерам.
При посещении одного и того же сайта один и тот же пользователь может как кликнуть,
так и не кликнуть по одному и тому же баннеру,
из-за чего в выборке могут появиться одинаковые объекты с разными ответами.
При этом важно, чтобы классификатор предсказывал именно вероятности классов~---
если домножить вероятность первого класса на сумму, которую заплатит заказчик в случае клика,
то мы получим матожидание прибыли при показе этого баннера.
На основе таких матожиданий можно построить алгоритм, выбирающий
баннеры для показа пользователю.

Итак, рассмотрим точку~$x$ пространства объектов.
Как мы договорились, в ней имеется распределение на ответах~$p(y = +1 \cond x)$.
Допустим, алгоритм~$b(x)$ возвращает числа из отрезка~$[0, 1]$.
Наша задача~--- выбрать для него такую процедуру обучения, что в точке~$x$
ему будет оптимально выдавать число~$p(y = +1 \cond x)$.
Если в выборке объект~$x$ встречается~$n$ раз с ответами~$\{y_1, \dots, y_n\}$,
то получаем следующее требование:
\[
    \argmin_{b \in \RR}
    \frac{1}{n}
    \sum_{i = 1}^{n}
        L(y_i, b)
    \approx
    p(y = +1 \cond x).
\]
При стремлении~$n$ к бесконечности получим, что функционал стремится к матожиданию ошибки:
\[
    \argmin_{b \in \RR}
    \EE \left[
        L(y, b)
        \cond
        x
    \right]
    =
    p(y = +1 \cond x).
\]
На семинаре будет показано, что этим свойством обладает, например, квадратичная функция потерь~$L(y, z) = (y - z)^2$,
если в ней для положительных объектов использовать истинную метку~$y = 1$, а для отрицательных брать~$y = 0$.

Примером функции потерь, которая не позволяет оценивать вероятности, является модуль отклонения~$L(y, z) = |y - z|$.
Можно показать, что с точки зрения данной функции оптимальным ответом всегда будет либо ноль, либо единица.

Это требование можно воспринимать более просто.
Пусть один и тот же объект встречается в выборке 1000 раз,
из которых 100 раз он относится к классу $+1$, и 900 раз~--- к классу $-1$.
Поскольку это один и тот же объект, классификатор должен выдавать один ответ
для каждого из тысячи случаев.
Можно оценить матожидание функции потерь в данной точке по 1000 примеров при прогнозе~$b$:
\[
    \EE \biggl[
        L(y, b)
        \cond
        x
    \biggr]
    \approx
    \frac{100}{1000}
    L(1, b)
    +
    \frac{900}{1000}
    L(-1, b).
\]
Наше требование, по сути, означает, что оптимальный ответ
с точки зрения этой оценки должен быть равен~$1/10$:
\[
    \argmin_{b \in \RR} \left(
        \frac{100}{1000} L(1, b)
        +
        \frac{900}{1000} L(-1, b)
    \right)
    =
    \frac{1}{10}.
\]

\subsection{Правдоподобие и логистические потери}
Хотя квадратичная функция потерь и приводит к корректному оцениванию вероятностей,
она не очень хорошо подходит для решения задачи классификации.
Причиной этому в том числе являются и слишком низкие штрафы за ошибку~---
так, если объект положительный, а модель выдаёт для него вероятность первого класса~$b(x) = 0$,
то штраф за это равен всего лишь единице:~$(1 - 0)^2 = 1$.

Попробуем сконструировать функцию потерь из других соображений.
Если алгоритм~$b(x) \in [0, 1]$ действительно выдает вероятности, то
они должны согласовываться с выборкой.
С точки зрения алгоритма вероятность того, что в выборке встретится объект~$x_i$ с классом~$y_i$,
равна~$b(x_i)^{[y_i = +1]} (1 - b(x_i))^{[y_i = -1]}$.
Исходя из этого, можно записать правдоподобие выборки (т.е. вероятность получить такую выборку
с точки зрения алгоритма):
\[
    Q(a, X)
    =
    \prod_{i = 1}^{\ell}
        b(x_i)^{[y_i = +1]} (1 - b(x_i))^{[y_i = -1]}.
\]
Данное правдоподобие можно использовать как функционал для обучения алгоритма~---
с той лишь оговоркой, что удобнее оптимизировать его логарифм:
\[
    -\sum_{i = 1}^{\ell} \left(
        [y_i = +1] \log b(x_i)
        +
        [y_i = -1] \log (1 - b(x_i))
    \right)
    \to
    \min
\]

Данная функция потерь называется логарифмической~(log-loss).
Покажем, что она также позволяет корректно предсказывать вероятности.
Запишем матожидание функции потерь в точке~$x$:
\begin{align*}
    \EE \biggl[
        L(y, b)
        \cond
        x
    \biggr]
    &=
    \EE \biggl[
        -[y = +1] \log b - [y = -1] \log(1 - b)
        \cond
        x
    \biggr]
    =\\
    &=
    -p(y = +1 \cond x) \log b
    -
    (1 - p(y = +1 \cond x)) \log(1 - b).
\end{align*}
Продифференцируем по~$b$:
\begin{align*}
    \frac{\partial}{\partial b}
    \EE \biggl[
        L(y, b)
        \cond
        x
    \biggr]
    &=
    -\frac{p(y = +1 \cond x)}{b} + \frac{1 - p(y = +1 \cond x)}{1 - b}
    =
    0.
\end{align*}
Легко видеть, что оптимальный ответ алгоритма равен вероятности положительного класса:
\[
    b_* = p(y = +1 \cond x).
\]

\subsection{Логистическая регрессия}
Везде выше мы требовали, чтобы алгоритм~$b(x)$ возвращал числа из отрезка~$[0, 1]$.
Этого легко достичь, если положить~$b(x) = \sigma(\langle w, x \rangle)$,
где в качестве~$\sigma$ может выступать любая монотонно неубывающая функция
с областью значений~$[0, 1]$.
Мы будем использовать сигмоидную функцию: $\sigma(z) = \frac{1}{1 + \exp(-z)}$.
Таким образом, чем больше скалярное произведение~$\langle w, x \rangle$,
тем больше будет предсказанная вероятность.
Как при этом можно интерпретировать данное скалярное произведение?
Чтобы ответить на этот вопрос, преобразуем уравнение
\[
    p(y = +1 \cond x)
    =
    \frac{1}{1 + \exp(-\langle w, x \rangle)}.
\]
Выражая из него скалярное произведение, получим
\[
    \langle w, x \rangle
    =
    \log
    \frac{
        p(y = +1 \cond x)
    }{
        p(y = -1 \cond x)
    }.
\]
Получим, что скалярное произведение будет равно логарифму отношения
вероятностей классов~(log-odds).

Как уже упоминалось выше, при использовании квадратичной функции потерь
алгоритм будет пытаться предсказывать вероятности,
но данная функция потерь является далеко не самой лучшей,
поскольку слабо штрафует за грубые ошибки.
Логарифмическая функция потерь подходит гораздо лучше, поскольку не позволяет алгоритму
сильно ошибаться в вероятностях.

Подставим трансформированный ответ линейной модели в логарифмическую функцию потерь:
\begin{align*}
    -\sum_{i = 1}^{\ell} &\left(
        [y_i = +1]
        \log \frac{1}{1 + \exp(-\langle w, x_i \rangle)}
        +
        [y_i = -1]
        \log \frac{\exp(-\langle w, x_i \rangle)}{1 + \exp(-\langle w, x_i \rangle)}
    \right)
    =\\
    &=
    -\sum_{i = 1}^{\ell} \left(
        [y_i = +1]
        \log \frac{1}{1 + \exp(-\langle w, x_i \rangle)}
        +
        [y_i = -1]
        \log \frac{1}{1 + \exp(\langle w, x_i \rangle)}
    \right)
    =\\
    &=
    \sum_{i = 1}^{\ell}
        \log \left(
            1 + \exp(-y_i \langle w, x_i \rangle)
        \right).
\end{align*}
Полученная функция в точности представляет собой логистические потери,
упомянутые в начале.
Линейная модель классификации, настроенная путём минимизации данного функционала,
называется логистической регрессией.
Как видно из приведенных рассуждений, она оптимизирует
правдоподобие выборки и дает корректные оценки вероятности принадлежности к положительному классу.

\section{Метод опорных векторов}

Рассмотрим теперь другой подход к построению функции потерь,
основанный на максимизации зазора между классами.
Будем рассматривать линейные классификаторы вида
\[
    a(x) = \sign (\langle w, x \rangle + b), \qquad w \in \RR^d, b \in \RR.
\]

\subsection{Разделимый случай}
Будем считать, что существуют такие параметры~$w_*$ и~$b_*$,
что соответствующий им классификатор~$a(x)$ не допускает ни одной ошибки
на обучающей выборке.
В этом случае говорят, что выборка~\emph{линейно разделима}.

Пусть задан некоторый классификатор~$a(x) = \sign (\langle w, x \rangle + b)$.
Заметим, что если одновременно умножить параметры~$w$ и~$b$
на одну и ту же положительную константу,
то классификатор не изменится.
Распорядимся этой свободой выбора и отнормируем параметры так, что
\begin{equation}
\label{eq:svmNormCond}
    \min_{x \in X} | \langle w, x \rangle + b| = 1.
\end{equation}
Можно показать, что расстояние от произвольной точки~$x_0 \in \RR^d$ до гиперплоскости,
определяемой данным классификатором, равно
\[
    \rho(x_0, a)
    =
    \frac{
        |\langle w, x \rangle + b|
    }{
        \|w\|
    }.
\]
Тогда расстояние от гиперплоскости до ближайшего объекта обучающей выборки равно
\[
    \min_{x \in X}
    \frac{
        |\langle w, x \rangle + b|
    }{
        \|w\|
    }
    =
    \frac{1}{\|w\|} \min_{x \in X} |\langle w, x \rangle + b|
    =
    \frac{1}{\|w\|}.
\]
Данная величина также называется~\emph{отступом~(margin)}.

Таким образом, если классификатор без ошибок разделяет обучающую выборку,
то ширина его разделяющей полосы равна~$\frac{2}{\|w\|}$.
Известно, что максимизация ширины разделяющей полосы приводит
к повышению обобщающей способности классификатора~\cite{mohri12foundations}.
Вспомним также, что на повышение обобщающей способности направлена и регуляризация,
которая штрафует большую норму весов~--- а чем больше норма весов,
тем меньше ширина разделяющей полосы.

Итак, требуется построить классификатор, идеально разделяющий обучающую выборку
и при этом имеющий максимальный отступ.
Запишем соответствующую оптимизационную задачу,
которая и будет определять метод опорных векторов для линейно разделимой выборки~(hard margin support vector machine):
\begin{equation}
\label{eq:svmSep}
    \left\{
        \begin{aligned}
            & \frac{1}{2} \|w\|^2 \to \min_{w, b} \\
            & y_i \left(
                \langle w, x_i \rangle + b
            \right) \geq 1, \quad i = 1, \dots, \ell.
        \end{aligned}
    \right.
\end{equation}
Здесь мы воспользовались тем, что линейный классификатор дает правильный ответ
на объекте~$x_i$ тогда и только тогда, когда~$y_i (\langle w, x_i \rangle + b) \geq 0$.
Более того, из условия нормировки~\eqref{eq:svmNormCond} следует,
что~$y_i (\langle w, x_i \rangle + b) \geq 1$.

В данной задаче функционал является строго выпуклым, а ограничения линейными,
поэтому сама задача является выпуклой и имеет единственное решение.
Более того, задача является квадратичной и может быть решена крайне эффективно.

\subsection{Неразделимый случай}
Рассмотрим теперь общий случай, когда выборку
невозможно идеально разделить гиперплоскостью.
Это означает, что какие бы~$w$ и~$b$ мы не взяли,
хотя бы одно из ограничений в задаче~\eqref{eq:svmSep}
будет нарушено:
\[
    \exists x_i \in X:\
    y_i \left(
        \langle w, x_i \rangle + b
    \right) < 1.
\]
Сделаем эти ограничения <<мягкими>>, введя штраф~$\xi_i \geq 0$ за их нарушение:
\[
    y_i \left(
        \langle w, x_i \rangle + b
    \right) \geq 1 - \xi_i, \quad i = 1, \dots, \ell.
\]

Отметим, что если отступ объекта лежит между нулем и
единицей~($0 \leq y_i \left( \langle w, x_i \rangle + b \right) < 1$),
то объект верно классифицируется, но имеет ненулевой штраф~$\xi > 0$.
Таким образом, мы штрафуем объекты за попадание внутрь разделяющей полосы.

Величина~$\frac{1}{\|w\|}$ в данном случае называется~\emph{мягким отступом~(soft margin)}.
С одной стороны, мы хотим максимизировать отступ, с другой~--- минимизировать
штраф за неидеальное разделение выборки~$\sum_{i = 1}^{\ell} \xi_i$.
Эти две задачи противоречат друг другу: как правило, излишняя подгонка под
выборку приводит к маленькому отступу, и наоборот~--- максимизация отступа
приводит к большой ошибке на обучении.
В качестве компромисса будем минимизировать взвешенную сумму двух указанных величин.
Приходим к оптимизационной задаче,
соответствующей методу опорных векторов для линейно неразделимой выборки~(soft margin support vector machine)
\begin{equation}
\label{eq:svmUnsep}
    \left\{
        \begin{aligned}
            & \frac{1}{2} \|w\|^2 + C \sum_{i = 1}^{\ell} \xi_i \to \min_{w, b, \xi} \\
            & y_i \left(
                \langle w, x_i \rangle + b
            \right) \geq 1 - \xi_i, \quad i = 1, \dots, \ell, \\
            & \xi_i \geq 0, \quad i = 1, \dots, \ell.
        \end{aligned}
    \right.
\end{equation}
Чем больше здесь параметр~$C$, тем сильнее мы будем настраиваться на обучающую выборку.

Данная задача также является выпуклой и имеет единственное решение.

\subsection{Сведение к безусловной задаче}
В начале лекции мы планировали разобраться с двумя функциями потерь: логистической и кусочно-линейной.
Функционал логистической регрессии действительно имеет вид верхней оценки на долю неправильных ответов,
а вот задача метода опорных векторов~\eqref{eq:svmUnsep} пока выглядит совершенно иначе.
Попробуем свести её к задаче безусловной оптимизации.

Перепишем условия задачи:
\[
    \left\{
    \begin{aligned}
        &\xi_i \geq 1 - y_i (\langle w, x_i \rangle + b) \\
        &\xi_i \geq 0
    \end{aligned}
    \right.
\]
Поскольку при этом в функционале требуется, чтобы штрафы~$\xi_i$ были как можно меньше,
то можно получить следующую явную формулу для них:
\[
    \xi_i
    =
    \max(0,
        1 - y_i (\langle w, x_i \rangle + b)).
\]
Данной выражение для~$\xi_i$ уже учитывает в себе все ограничения задачи~\eqref{eq:svmUnsep}.
Значит, если подставить его в функционал, то получим безусловную задачу оптимизации:
\[
    \frac{1}{2} \|w\|^2
    +
    C
    \sum_{i = 1}^{\ell}
        \max(0,
            1 - y_i (\langle w, x_i \rangle + b))
    \to
    \min_{w, b}
\]
Эта задача является негладкой, поэтому решать её может быть достаточно тяжело.
Тем не менее, она показывает, что метод опорных векторов, по сути,
тоже строит верхнюю оценку вида~$L(y, z) = \max(0, 1 - yz)$ на долю ошибок,
и добавляет к ней стандартную квадратичную регуляризацию.


\begin{thebibliography}{1}
\bibitem{mohri12foundations}
    \emph{Mohri, M., Rostamizadeh, A., Talwalkar, A.}
    Foundations of Machine Learning.~// MIT Press, 2012.
\end{thebibliography}
\end{document}
