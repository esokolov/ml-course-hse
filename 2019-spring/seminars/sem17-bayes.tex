\documentclass[12pt,fleqn]{article}
\usepackage{../lecture-notes/vkCourseML}
\hypersetup{unicode=true}
\usepackage{xcolor}
%\usepackage[a4paper]{geometry}
\usepackage{cases}

\interfootnotelinepenalty=10000
\title{Машинное обучение, ФКН ВШЭ\\Семинар №17}
\author{}
\date{}
\begin{document}
	\maketitle

\section{Байесовские методы машинного обучения}
Пусть~$X = \{x_1, \dots, x_\ell\}$~--- выборка,
$\XX$~--- множество всех возможных объектов,
$Y$~--- множество ответов.
В байесовском подходе предполагается, что обучающие
объекты и ответы на них~$(x_1, y_1), \dots, (x_\ell, y_\ell)$ независимо выбираются
из некоторого распределения~$p(x, y)$, заданного на множестве~$\XX \times Y$.
Данное распределение можно переписать как
\[
    p(x, y)
    =
    p(y) p(x \cond y),
\]
где~$p(y)$ определяет вероятности появления каждого из возможных ответов
и называется~\emph{априорным распределением},
а~$p(x \cond y)$ задает распределение объектов при фиксированном ответе~$y$
и называется~\emph{функцией правдоподобия}.

Если известны априорное распределение и функция правдоподобия,
то по формуле Байеса можно записать~\emph{апостериорное распределение}
на множестве ответов:
\[
    p(y \cond x)
    =
    \frac{
        p(x \cond y) p(y)
    }{
        \int_s p(x \cond s) p(s) ds
    }
    =
    \frac{
        p(x \cond y) p(y)
    }{
        p(x)
    },
\]
где знаменатель не зависит от~$y$ и является нормировочной константой.

\subsection{Оптимальные байесовские правила}
Пусть на множестве всех пар ответов~$Y \times Y$ задана функция
потерь~$L(y, s)$.
Наиболее распространенным примером для задач классификации
является ошибка классификации~$L(y, s) = [y \neq s]$,
для задач регрессии~--- квадратичная функция потерь~$L(y, x) = (y - s)^2$.
\emph{Функционалом среднего риска} называется матожидание функции потерь
по всем парам~$(x, y)$ при использовании алгоритма~$a(x)$:
\[
    R(a) = \EE L(y, a(x))
    =
    \int_{Y} \int_{\XX} L(y, a(x)) p(x, y) dx dy.
\]
Если распределение~$p(x, y)$ известно, то можно найти алгоритм~$a_*(x)$,
оптимальный с точки зрения функционала среднего риска.

\subsubsection{Классификация}
Начнем с задачи классификации с множеством ответом~$Y = \{1, \dots, K\}$
и функции потерь~$L(y, s) = [y \neq s]$.
Покажем, что минимум функционала среднего риска достигается
на алгоритме
\[
    a_*(x) = \argmax_{y \in Y} p(y \cond x).
\]

Для произвольного классификатора~$a(x)$ выполнена
следующая цепочка неравенств:
\begin{align*}
    R(a)
    &=
    \int_{Y} \int_{\XX} L(y, a(x)) p(x, y) dx dy
    =
    \\
    &=
    \sum_{y = 1}^{K} \int_{\XX} [y \neq a(x)] p(x, y) dx
    =
    \\
    &=
    \int_{\XX} \sum_{y \neq a(x)} p(x, y) dx
    =
    \left\{
    \int_{\XX} \sum_{y \neq a(x)} p(x, y) dx
    +
    \int_{\XX} p(x, a(x)) dx
    =
    1
    \right\}
    =
    \\
    &=
    1 - \int_{\XX} p(x, a(x)) dx
    \geq\\
    &\geq
    1 - \int_{\XX} \max_{s \in Y} p(x, s) dx
    =
    \\
    &=
    1 - \int_{\XX} p(x, a_*(x)) dx
    =
    \\
    &=
    R(a_*)
\end{align*}
Таким образом, средний риск любого классификатора~$a(x)$
не превосходит средний риск нашего классификатора~$a_*(x)$.

Мы получили, что оптимальный байесовский классификатор
выбирает тот класс, который имеет наибольшую апостериорную вероятность.
Такой классификатор называется~\emph{MAP-классификатором} (maximum a posteriori).

\subsubsection{Регрессия}

Напомним, что при выводе разложения на шум, смещение и разброс функционала среднего риска для задачи регрессии и функции потерь~$L(y, x) = (y - s)^2$ нами уже была получена формула оптимального алгоритма с точки зрения данного функционала:
\[
    a_*(x) = \EE (y \cond x)
    =
    \int_Y y p(y \cond x) dy.
\]
Иными словами, мы должны провести <<взвешенное голосование>>
по всем возможным ответам, причем вес ответа равен его
апостериорной вероятности.

\subsection{Особенности байесовских алгоритмов}
Основной проблемой оптимальных байесовских алгоритмов,
о которых шла речь в предыдущем разделе, является
невозможность их построения на практике, поскольку нам никогда
неизвестно распределение~$p(x, y)$.
Данное распределение можно попробовать восстановить по обучающей выборке,
при этом существует два подхода~--- параметрический и непараметрический.
Сейчас мы сосредоточимся на параметрическом подходе.

Допустим, распределение на парах~<<объект-ответ>> зависит от
некоторого параметра~$\theta$: $p(x, y \cond \theta)$.
Тогда получаем следующую формулу для апостериорной вероятности:
\[
    p(y \cond x, \theta)
    \propto
    p(x \cond y, \theta) p(y),
\]
где выражение~<<$a \propto b$>> означает~<<$a$ пропорционально~$b$>>.
Для оценивания параметров применяется~\emph{метод максимального правдоподобия}:
\[
    \theta_*
    =
    \argmax_\theta
        L(\theta)
    =
    \argmax_\theta
        \prod_{i = 1}^{\ell} p(x_i \cond y_i, \theta),
\]
где $L(\theta)$~--- функция правдоподобия.
Примером такого подхода может служить~\emph{нормальный дискриминантный анализ},
где предполагается, что функции правдоподобия являются нормальными распределениями:
\begin{align*}
    &a(x) = \argmax_{y \in Y} p(y) p(x \cond y),\\
    &p(x \cond y) = \NN(x \cond \mu_y, \Sigma_y).
\end{align*}
Параметрами алгоритма являются средние~$\mu_y$ и
ковариационные матрицы классов~$\Sigma_y$,
которые оцениваются по выборке методом максимального
правдоподобия.

	\par Если предположить, что ковариационные матрицы классов равны,
и оценивать их по всей выборке, то мы получим алгоритм,
называемый~\emph{линейным дискриминантом Фишера}.
Можно показать, что он является линейным:
\[
    a(x)
    =
    \argmax_{y \in Y} ( \langle w_y, x \rangle + w_{0y} ),
\]
причем~$w_y = \Sigma^{-1} \mu_y$.
В случае двух классов~($Y = \{-1, +1\}$) классификатор принимает вид
\begin{equation}
\label{eq:ldaClassifier}
    a(x)
    =
    \sign \left(
        \langle w, x \rangle + b
    \right)
    \quad
    w = \Sigma^{-1} (\mu_2 - \mu_1).
\end{equation}

\subsection{Наивный байесовский классификатор}
\par Как было сказано ранее, при применении байесовского классификатора необходимо решить задачу восстановления плотности $p_y(x)$ для каждого класса $y \in \mathbb{Y}.$ Данная задача является довольно трудоёмкой и не всегда может быть решена, особенно в случае большого количества признаков, — в частности, если объектами являются тексты, приходится работать с крайне большим числом признаков, и восстановление плотности многомерного распределения не представляется возможным.
\par Для разрешения этой проблемы сделаем предположение о независимости признаков. В этом случае функция правдоподобия класса $y$ для объекта $x = \left( x_1, \dots, x_d\right)$ может быть представлена в следующем виде:
\begin{align*}
	p(x \cond y) = \prod_{j=1}^d p(x_j \cond y), 
\end{align*}
где $p(x_j \cond y)$ — одномерная плотность распределения $j$-ого признака объектов класса~$y \in Y.$ В этом случае формула байесовского решающего правила примет следующий вид:
\begin{align*}
	a(x) = \arg \max_{y \in Y} p(y \cond x) =
\arg \max_{y \in \mathbb{Y}} \left( \ln p(y) + \sum_{j=1}^d \ln p(x_j \cond y) \right).
\end{align*}

Предположение о независимости признаков существенно облегчает задачу, поскольку вместо решения задачи восстановления $d$-мерной плотности необходимо решить $d$ задач восстановления одномерных плотностей. Полученный классификатор называется \emph{наивным байесовским классификатором}.
\par Плотности отдельных признаков могут быть восстановлены различными способами (параметрическими и непараметрическими). Среди параметрических способов чаще всего используются нормальное распределение (для вещественных признаков), распределение Бернулли и мультиномиальное распределение (для дискретных признаков), благодаря которым получаются различные применяющиеся на практике модели.

\end{document} 
