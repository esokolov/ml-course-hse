\documentclass[12pt,a4paper]{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{ucs}
\usepackage{float}
\usepackage[utf8x]{inputenc}
\usepackage[russian]{babel}
\usepackage{hyphenat}
\usepackage{amsmath}
\usepackage{tabularx} 
\usepackage{adjustbox} 
\usepackage{makecell} 
\usepackage{listings}
\usepackage{xcolor}
\usepackage{enumitem}
\usepackage{pgfplotstable}
\usepackage{graphicx}
\usepackage{pgfplots}
\usepackage{colortbl}
%\usepackage{../lecture-notes/vkCourseML}
%\usepackage{lipsum}
%\usepackage{indentfirst}
\title{Машинное обучение, ФКН ВШЭ\\Семинар №11}
\author{}
\date{}
\begin{document}
\maketitle

\section{Когда нужна интерпретация?}

При решении любой ML-задачи есть выбор — использовать интерпретируемую по определению модель или нет. 
В первом случае методы интерпретации могут оказаться избыточны, во втором случае методы интерпретации необходимы. 
С их помощью мы анализируем, как признаки влияют на прогноз в целом и точечно.

Существует множество способов разделить методы интерпретации между собой. Основные способы разделения, это:

\paragraph{Ante-hoc и Post-hoc}  
\begin{itemize}
    \item \textbf{Ante-hoc} (или \textbf{интерпретируемые по определению}) — это модели, в которых интерпретация заложена изначально в структуру. Их преимущество в том, что объяснение получается в комплекте  с моделью.
    \item \textbf{Post-hoc} — это методы, которые применяются уже после обучения модели, когда сама структура модели сложна (такие модели называют моделями "черного ящика"(например, нейросети или иногда бустинги)).  
\end{itemize}

\paragraph{Локальная и глобальная интерпретация}  
\begin{itemize}
    \item \textbf{Локальная интерпретация} — объяснение конкретного предсказания.  Отвечает на вопрос: «Почему модель приняла такое решение для данного объекта?»  
    \item \textbf{Глобальная интерпретация} — анализ модели в целом: какие признаки наиболее важны, как они взаимодействуют между собой, как их изменение влияет на результат в среднем.  
\end{itemize}

\paragraph{Model specific и Model agnostic}  
\begin{itemize}
    \item \textbf{Model specific} — методы, предназначенные для конкретных классов моделей.  
    \item \textbf{Model agnostic} — универсальные методы, которые можно применять к любой модели («чёрному ящику»).  
\end{itemize}

Приложение полученных выводов может быть разным, в зависимости от предметной области.

Например, мы знаем:
\begin{itemize}
    \item какие показатели здоровья для конкретного пациента привели к тому, что он отнесён в группу риска. 
    Эта информация может быть использована для верификации и интерпретации врачом;
    \item какие показатели привели к отказу от кредита клиенту. Тогда, поскольку по закону каждый человек имеет 
    \emph{право на объяснение}, если решения по отношению к нему принимаются ИИ, интерпретация позволяет обеспечить это право;
    \item какие показатели привели к повышению вероятности найма для сотрудника, автоматически отобранного по резюме. 
    Тогда возможно \emph{верифицировать показатели на предвзятость};
    \item какие показатели привели к тому, что оборудование было помечено как ломающееся (и впоследствии вышло из строя). 
    Зная, что повлияло на поломку, сотрудник может быстрее локализовать и исправить проблему.
\end{itemize}

В общем же, в литературе выделяют четыре основных направления приложений XAI:
\begin{enumerate}
    \item повышение доверия к ИИ;
    \item обеспечение согласованности алгоритмов с человеческими ожиданиями и законодательством;
    \item извлечение новых знаний из предметной области или генерация новых гипотез для бизнеса;
    \item дебаггинг модели.
\end{enumerate}

\section{Интерпретация по определению}

К интерпретируемым по определению (моделям, для которых интерпретация ante-hoc) относят модели, чья структура и параметры имеют математический смысл. К таким моделям относятся:

\subsection*{Линейная регрессия и обобщённые линейные модели}
Классическая модель имеет вид:
\[
y = \beta_0 + \beta_1 x_1 + \ldots + \beta_p x_p + \varepsilon,
\]
где каждый коэффициент $\beta_j$ показывает изменение отклика $y$ при увеличении признака 
$x_j$ на единицу при прочих равных.  
Благодаря этому коэффициенты имеют ясную количественную интерпретацию.

\subsection*{Логистическая регрессия}
Модель описывает логарифм отношения шансов:
\[
\log \frac{P(y=1)}{P(y=0)} = \beta_0 + \beta_1 x_1 + \ldots + \beta_p x_p,
\]
и коэффициенты $\beta_j$ интерпретируются как вклад признака $x_j$ в изменение шансов.  
Это делает модель прозрачной даже для задач бинарной классификации.

\subsection*{Наивный Байесовский классификатор}
Модель основана на формуле Байеса:
\[
P(y \mid x_1, \ldots, x_p) = P(y)\prod_{j=1}^p P(x_j \mid y).
\]
Интерпретация строится на том, что каждое предсказание определяется априорной вероятностью 
класса $P(y)$ и вкладом каждого признака $x_j$ через условные вероятности $P(x_j \mid y)$.  
«Наивное» допущение независимости признаков позволяет прозрачно показать, 
какой именно фактор усиливает или ослабляет вероятность класса.

\subsection*{Решающие деревья}
С одной стороны, дерево решений представляет собой иерархию правил вида «если–то», 
где на каждом шаге осуществляется пороговое разбиение признака.  
Интерпретация здесь заключается в том, что каждое предсказание можно объяснить конкретным маршрутом по дереву и набором условий.

С другой стороны, каждый узел в дереве строится как решение задачи уменьшения меры загрязнённости.  
Важность признака в узле $N_k$ вычисляется как:
\[
I_k = \frac{w_k \cdot impurity_k \;-\; w_{left(k)} \cdot impurity_{left} \;-\; w_{right(k)} \cdot impurity_{right}}{100},
\]
где:
\begin{itemize}[noitemsep]
    \item $w_k$ — доля наблюдений в узле от исходных данных,
    \item $impurity_k$ — мера загрязнённости (например, индекс Джини или энтропия),
    \item $w_{left(k)}, w_{right(k)}$ и $impurity_{left}, impurity_{right}$ — 
    аналогичные величины для дочерних узлов.
\end{itemize}

Общая важность признака вычисляется как сумма по всем узлам, где он использовался, 
нормированная на общую важность всех признаков:
\[
FeatureImportance(f_k) = 
\frac{\sum_{N_k: f=f_k} I_k}{\sum_{j} \sum_{N_k} I_k}.
\]

Таким образом, интерпретация дерева по определению возможна как на:
\begin{itemize}
    \item \textbf{локальном уровне} — через правило для конкретного объекта,
    \item \textbf{глобальном уровне} — через сравнение важностей признаков.
\end{itemize}

Существуют также сопосбы дизайна интерпретируемых DNN. Но это отдельная область исследований. 

\section{Графические (plot-based) методы}

Название \emph{plot-based} для класса методов, с которыми мы ознакомимся ниже, 
не является строгим или формальным. Оно скорее отражает смысл: 
наиболее информативная часть этих методов сосредоточена в их графическом представлении. Эти методы были одними из первых в силу развития области — все графические методы удобны на табличных данных, которые были наиболее распространены в началах дисциплины машинного обучения. Разберем те, которые применяются по сегодняшний день — ICE, PDP и ALE. 

\subsection{ICE: Individual Conditional Expectation}

\textbf{Individual Conditional Expectation (ICE)} — графики индивидуального условного ожидания.  
Задача метода: оценить, как меняется прогноз модели с изменением конкретного признака 
для каждого объекта в некотором наборе $X_{test}$.

\paragraph{Построение:}
\begin{enumerate}
    \item Зафиксируем множество $X_{test}$ и некоторый признак $j$. Пусть $j$ имеет $m$ уникальных значений $[j_1, j_2, \ldots, j_m]$.
    \item Исходный датасет $X_{test}$ дублируем $m$ раз: 
    \[
    X'_1, X'_2, \ldots, X'_m,
    \] 
    так, что для датасета $X'_i$ значение признака $j$ фиксируется равным $j_i$.
    \item На каждом $X'_i$ рассчитываем прогноз модели $f(X'_i)$.
    \item На графике строим линии для каждого объекта, показывающие, 
    как прогноз (ось $y$) меняется при изменении признака (ось $x$).
\end{enumerate}

ICE реализован в \texttt{scikit-learn}.  
Метод даёт \textbf{локальное объяснение} — для отдельных объектов.


\subsection{PDP: Partial Dependence Plot}

Следующий тип графической оценки важностей — график частичной зависимости (Partial Dependence Plot, PDP).  
Он является логическим продолжением ICE: фактически, это усреднённый ICE-график. Смысл усреднения показан на рисунке~\ref{fig:ice-pdp}.
Усредняя ICE, PDP представляет собой \textbf{глобальную интерпретацию}.

\paragraph{Формализация.}  
Пусть:
\begin{itemize}
    \item $X$ — набор данных, состоящий из $n$ наблюдений, описанных $m$ признаками,
    \item $x_s$ и $x_c$ — два множества признаков, где $x_s$ содержит признаки, важность которых мы исследуем, 
    а $x_c$ — остальные признаки. Тогда $x_s \cup x_c = x$.
\end{itemize}

Функция частичной зависимости имеет вид:
\[
PD(x_s) = \int \hat{f}(x_s, x_c)\, dP(x_c),
\]
где $\hat{f}()$ — обученная модель, а $P(x_c)$ — распределение остальных признаков.

При $|x_s|=1$ эта формула сводится к усреднению ICE по всем объектам.  
PDP также реализован в \texttt{scikit-learn}.

\begin{center}
\begin{figure}[!ht]
\centering
\includegraphics[width=1\linewidth]{img_xai/ice_pdp.png}
\caption{ICE (голубые линии) и PDP (пунктир)}\label{fig:ice-pdp}
\end{figure}
\end{center}


\subsection{ALE: Accumulated Local Effects}

Заключительный из основных графических методов — \textbf{Accumulated Local Effects (ALE)} (рис. ~\ref{fig:ale}.  
Метод был предложен в 2019 году как альтернатива PDP для случаев мультиколлинеарных признаков.

\paragraph{Формализация.} Пусть:
\begin{itemize}
    \item $f(x)$ — предсказания модели,
    \item $F$ — интересующий нас признак,
    \item $S_j$ — все остальные признаки, кроме $F$,
    \item $[x_F^{(k)}, x_F^{(k+1)}]$ — интервалы значений признака $F$, где $k=1,2,\ldots,K-1$,
    \item $N_j(k)$ — число наблюдений, попавших в интервал $k$.
\end{itemize}

Тогда эффект ALE вычисляется как:
\[
f_{F, ALE} = \sum_{k=1}^K \frac{1}{|N_j(k)|} \sum_{x \in N_j(k)} \big[f(x_F^{(k+1)}, S_j) - f(x_F^{(k)}, S_j)\big].
\]

Чтобы центрировать эффект относительно среднего по всем наблюдениям:
\[
f_{F, centeredALE} = f_{F, ALE} - \frac{1}{n}\sum_{k=1}^K N_j(k) f_{F, ALE}.
\]

\paragraph{Алгоритм построения.}
\begin{enumerate}
    \item Выбрать интересующий признак $F$.
    \item Разделить диапазон его значений на интервалы.
    \item Для каждого интервала:
    \begin{itemize}
        \item[a)] подставить нижнюю границу $x_F^{(k)}$ и рассчитать прогноз $f(x_F^{(k)}, S_j)$;
        \item[б)] подставить верхнюю границу $x_F^{(k+1)}$ и рассчитать прогноз $f(x_F^{(k+1)}, S_j)$;
        \item[в)] усреднить разницу прогнозов по наблюдениям интервала.
    \end{itemize}
    \item Суммировать эффекты по интервалам и центрировать.
\end{enumerate}

ALE также является \textbf{глобальным методом}, менее чувствительным к мультиколлинеарности, чем PDP.

\begin{center}
\begin{figure}[!ht]
\centering
\includegraphics[width=1\linewidth]{img_xai/ale.png}
\caption{AlE plot, pyALE (https://github.com/DanaJomar/PyALE)}\label{fig:ale}
\end{figure}
\end{center}

\section{Permutation Importances}

Следующий класс методов дает для оценки важности коэффициенты. Это позволяет сравнивать важность признаков между собой в числовом формате, что удобнее и нагляднее, чем визуально. Разберем 3 метода — универсальных и применяемых в настощее время, в том числе для DNN. 

Первым разберем Permutation Importances. Идея метода такова — если признак значим, то случайные перестановки признака сильно ухудшат производительость модели. 

Формально, пусть $a(x)$ — модель, обученная на множестве признаков 
$F = \{f_1, f_2, \ldots , f_n\}$, а $e_{orig}$ — ошибка модели на тестовом наборе данных.  
Зафиксируем признак $f_i$. Для него выполняются следующие шаги:

\begin{enumerate}
    \item Случайным образом переставим его значения в тестовом наборе данных.
    \item Вычислим прогноз модели на тестовых данных с перестановкой.
    \item Оценим ошибку модели на наборе данных с перетасовкой $e_{perm}$.
    \item Оценим важность признака как разность $e_{orig} - e_{perm}$ или отношение $\tfrac{e_{perm}}{e_{orig}}$.
\end{enumerate}

Алгоритм повторяется для всех признаков, после чего они сортируются по убыванию важности.  

Общая важность признака $f_i$ определяется как:
\[
Importance(f_i) = e_{orig} - \frac{1}{K}\sum_{k=1}^K e_{perm, k},
\]
где $K$ — число перестановок.
\begin{center}
\begin{figure}[!ht]
\centering
\includegraphics[width=1\linewidth]{img_xai/perm.png}
\caption{Пример использования оценок, полученных из перестановочной важности — построение изменений метрики и анализ разброса, позволяющий оценить силу и устойчивость влияния перестановки на признак.}\label{fig:perm}
\end{figure}
\end{center}

\section{SHAP}

Метод SHAP (SHapley Additive exPlanations) — один из самых популярных методов. Популярность обусловлена универсальностью — используя его основу можно интерпретировать любые модели — от моделей машинного до моделей глубинного обучения, обученных при том на разных модальностях данных (текст, картинки, таблицы и даже геномные данные). 

Сами значения Шепли были предложены ещё в 1951г. и изначально они не связаны с машинным обучением. Они пришли в область из задач теории кооперативных игр. Поэтому, чтобы ввести метод, нам понадобится ряд всопомгательных определений. 

Пусть $N = \{1, \ldots, n\}$ — конечное множество игроков.  \\
Любое подмножество $S \subset N$ называется \textbf{коалицией}, 
а само $N$ — \textbf{гранд-коалицией}. \\ 
Пару $(S, v)$ будем называть \textbf{кооперативной игрой}, 
где $v : 2^N \to \mathbb{R}$ — характеристическая функция игры.\\

В контексте интерпретации признаков:
\begin{itemize}
    \item \textbf{Игроки} — это признаки.
    \item \textbf{Коалиции} — это подмножества признаков, используемых для прогноза.
    \item \textbf{Характеристическая функция} $v(S)$ — прогноз модели на подмножестве $S$ признаков.
\end{itemize}

\textbf{Вычисление значения Шепли.}

Чтобы оценить вклад признака $i$:
\begin{enumerate}
    \item Рассматриваем все подмножества $S \subseteq N \setminus \{i\}$ без игрока $i$ и считаем прогноз $v(S)$.
    \item Добавляем игрока $i$ и считаем прогноз $v(S \cup \{i\})$.
    \item Берём разность:
    \[
    \Delta(i,S) = v(S \cup \{i\}) - v(S).
    \]
\end{enumerate}

Значение Шепли для игрока $i$ определяется как:
\[
Sh(v)_i = \sum_{S \subseteq N \setminus \{i\}}
\frac{|S|!(n-|S|-1)!}{n!}\,\big(v(S \cup \{i\}) - v(S)\big).
\]

Здесь:
\begin{itemize}
    \item $\Delta(i,S) = v(S \cup \{i\}) - v(S)$ — прирост от добавления игрока $i$ в коалицию $S$,
    \item $\tfrac{|S|!(n-|S|-1)!}{n!}$ — нормирующий множитель (учитывает количество перестановок игроков).
\end{itemize}

\paragraph{Практические особенности}
Основная слабая черта метода — вычислительная сложность.  
Для $n$ признаков требуется перебор всех $2^n$ подмножеств, 
что делает точный расчёт невозможным при большом $n$.  
На практике используются приближённые алгоритмы (например, KernelSHAP, TreeSHAP). Однако, вопреки недостаткам — метод испольузется чаще всего в силу утойчивости и теоретического фундмаента. Кроме того, он дает возможность строить как локальные так и глобальные объяснения. 

\begin{center}
\begin{figure}[!ht]
\centering
\includegraphics[width=0.7\linewidth]{img_xai/shap_global.png}
\caption{Пример глобальной интерпретации с SHAP}\label{fig:shap}
\end{figure}
\end{center}

\begin{center}
\begin{figure}[!ht]
\centering
\includegraphics[width=0.65\linewidth]{img_xai/shap_local.png}
\caption{Пример локальной интерпретации с SHAP}\label{fig:shap}
\end{figure}
\end{center}

\section{LIME}

\textbf{LIME (Local Interpretable Model-agnostic Explanations)} — метод пост-хок интерпретации, 
основанный на аппроксимации сложной модели $f(x)$ простой и интерпретируемой моделью $g(z)$ 
в локальной окрестности точки $x$.  

\paragraph{Основная идея}
Вместо того чтобы интерпретировать модель $f$ глобально, 
мы обучаем \emph{суррогатную модель} $g \in G$ (обычно линейную или деревообразную), 
которая аппроксимирует поведение $f$ в малой окрестности точки $x$.

Формально решается задача оптимизации:
\[
\xi(x) = \arg\min_{g \in G} \Big(L(f, g, \pi_x) + \Omega(g)\Big),
\]
где:
\begin{itemize}
    \item $L(f, g, \pi_x)$ — функция потерь, измеряющая, 
    насколько хорошо $g$ приближает $f$ в окрестности $x$;
    \item $\pi_x$ — весовая функция, задающая близость объектов к точке $x$;
    \item $\Omega(g)$ — штраф за сложность суррогатной модели, 
    обеспечивающий её интерпретируемость.
\end{itemize}

\begin{center}
\begin{figure}[!ht]
\centering
\includegraphics[width=1\linewidth]{img_xai/lime_tab.png}
\caption{LIME}\label{fig:lime}
\end{figure}
\end{center}

\paragraph{Функция потерь}
Функция потерь имеет вид:
\[
L(f, g, \pi_x) = \pi_x \big(f(z) - g(x')\big)^2,
\]
где:
\begin{itemize}
    \item $f(z)$ — предсказание исходной модели для объекта $z$,
    \item $g(x')$ — предсказание суррогатной модели в пространстве интерпретируемых признаков,
    \item $\pi_x$ — коэффициент, зависящий от расстояния между $z$ и точкой $x$.
\end{itemize}

Таким образом, объекты, близкие к $x$, получают больший вес при обучении $g$, 
а удалённые — меньший и в общем LIME позволяет понять, какие признаки повлияли на конкретное предсказание модели $f$ и в какой степени. В отличие от глобальных методов, он даёт локальное объяснение, хотя существуют расширения для глобальной интерпретации. 
\end{document}