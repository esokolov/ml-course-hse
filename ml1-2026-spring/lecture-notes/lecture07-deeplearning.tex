\documentclass[a4paper,12pt]{article}
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{float}
\usepackage[hidelinks]{hyperref}
\usepackage{xurl} 
\urlstyle{same}


\geometry{left=2.5cm, right=2.5cm, top=2.5cm, bottom=2.5cm}

\newtheorem{definition}{Определение}
\newtheorem{remark}{Замечание}
\newtheorem{theorem}{Теорема}

\begin{document}
\title{Лекция 7\\Введение в глубинное обучение}
\author{Е.\,А.\,Соколов\\ФКН ВШЭ}
\maketitle

\section{Мотивация}

Ранее мы подробно изучили линейные модели классификации и регрессии. Мы выяснили, что они обладают множеством преимуществ: скоростью обучения, интерпретируемостью и простотой реализации. Однако мы также столкнулись с их существенным недостатком - ограниченной выразительной способностью. Линейный классификатор может построить только гиперплоскость, что часто оказывается недостаточным для сложных данных.

В этой лекции мы перейдем к методам глубинного обучения (Deep Learning). Мы попытаемся понять, как можно преодолеть ограничения линейных моделей, выстраивая композиции из простых преобразований, и обсудим основные строительные блоки современных нейронных сетей.

Основная проблема, с которой мы сталкиваемся при использовании линейных моделей на сложных данных (например, изображениях или текстах), заключается в том, что зависимость между признаками и целевой переменной редко бывает линейной. До сих пор у нас было два пути решения этой проблемы:
\begin{enumerate}
    \item Конструирование признаков (Feature Engineering). Мы могли вручную придумывать новые признаки (полиномиальные, комбинированные и т.п.), чтобы улучшить разделимость классов в пространстве признаков (вплоть до линейной разделимости).
    \item Композиции моделей. Использование бустинга или бэггинга над простыми алгоритмами.
\end{enumerate}

Глубинное обучение - это по сути систематизация второго пути: вместо ручной инженерии признаков мы строим модель как последовательность простых преобразований.

Определение глубинного обучения. Построение сложных моделей путем суперпозиции (композиции) простых базовых функций или слоев:
$$a(x)=b_1\big(b_2(\dots b_n(x)\dots)\big),$$
где $x$ - входной объект, а $b_i$ - функции слоя.

Такой подход имеет ряд преимуществ:
\begin{itemize}
    \item Легкость дифференцирования: благодаря правилу дифференцирования сложной функции (chain rule), мы можем обучать такие конструкции методом градиентного спуска.
    \item Систематичность: это дает нам подход к построению моделей любой сложности, собирая их как конструктор из готовых блоков.
\end{itemize}

\section{Полносвязные слои (Fully Connected)}

\subsection{Определение и математическая модель}
Базовым строительным блоком нейронных сетей является полносвязный слой. По сути, это уже знакомое нам линейное преобразование, но с несколькими выходами.

Определение полносвязных сетей. Линейное преобразование, которое отображает входной вектор $x \in \mathbb{R}^n$ в вектор $z \in \mathbb{R}^m$. Каждый элемент выходного вектора $z_j$ (где $j=1,\dots,m$) представляет собой взвешенную сумму всех входных признаков со сдвигом:
$$z_j = \sum_{i=1}^{n} w_{ji} x_i + b_j.$$

В матричном виде это записывается компактно:
$$z = Wx + b,$$
где $W \in \mathbb{R}^{m \times n}$ - матрица весов, а $b \in \mathbb{R}^m$ - вектор сдвигов. Заметим, что каждый выход является линейной комбинацией \textbf{всех} входов.

\subsection{Аналогия из биологии}
Часто структуру нейронных сетей объясняют через аналогию с биологическим мозгом. Рассмотрим упрощенную математическую модель нейрона:
\begin{itemize}
    \item Дендриты соответствуют входным сигналам $x_i$.
    \item Синапсы моделируются весами $w_i$, которые могут усиливать или ослаблять сигнал.
    \item Тело клетки производит суммирование взвешенных сигналов.
    \item Аксон передает результат дальше следующему нейрону.
\end{itemize}

Эта модель в точности соответствует одному выходу полносвязного слоя. Однако стоит помнить, что это лишь \textit{грубая математическая абстракция}. Современные представления о биологических нейронах намного сложнее: они описываются дифференциальными уравнениями, обладают временными задержками и сложной химической природой передачи сигнала.

\subsection{Проблемы линейности}
Если мы попробуем построить сеть, просто накладывая полносвязные слои друг на друга (например, $z = W_2(W_1 x + b_1) + b_2$), мы столкнемся с определенной проблемой.

Из линейной алгебры известно, что композиция линейных преобразований сама является линейным преобразованием. Раскрыв скобки, мы получим, что такая \textit{глубокая} сеть эквивалентна одному-единственному полносвязному слою с другими весами. Это возвращает нас к исходной проблеме ограниченной выразительности.

Полносвязные слои также страдают от чрезмерного количества параметров. Для слоя с $n$ входами и $m$ выходами нам нужно хранить $n \times m$ весов, что при работе с высокоразмерными данными приводит к вычислительным сложностям.

\section{Нелинейность и функции активации}

Чтобы нейронная сеть могла восстанавливать сложные зависимости, необходимо внедрить нелинейность между линейными слоями.

Функция активации - это нелинейная функция $\sigma(z)$, которая применяется поэлементно к выходу линейного слоя:
$$y = \sigma(Wx + b)$$

Рассмотрим наиболее популярные функции активации.

\subsection{ReLU (Rectified Linear Unit)}
На данный момент это \textit{стандарт по умолчанию} в глубинном обучении.

$$ReLU(x) = \max(0, x)$$

У этой функции активации есть как преимущества, так и недостатки. Сильные стороны:
\begin{itemize}
    \item Вычислительная эффективность (простое сравнение с нулем).
    \item Простая производная (0 или 1), что облегчает обучение и спасает от проблемы затухания градиентов.
\end{itemize}
Слабые стороны:
\begin{itemize}
    \item Проблема \textit{умирающих нейронов} (Dying ReLU). Если вход нейрона уходит в отрицательную область, градиент становится равным нулю, и веса перестают обновляться.
\end{itemize}

\subsection{Swish}
Есть несколько вариаций, как можно улучшить свойства ReLU. Например, Swish.

$$Swish(x) = x \cdot \sigma(\beta x),$$
где $\sigma$ - сигмоида, а $\beta$ - обучаемый параметр.

В отличие от ReLU, Swish является гладкой функцией и не равна нулю при отрицательных значениях (при малых $\beta$), что позволяет градиентам протекать даже через отрицательные активации.

\section{Теорема об универсальной аппроксимации}

Возникает естественный вопрос: насколько мощным инструментом являются нейронные сети?

В этом вопросе мы обратимся к теореме Цыбенко (1989): для любой непрерывной функции $f: [0, 1]^d \rightarrow \mathbb{R}$ и любого $\epsilon > 0$ существует двухслойная нейронная сеть (с одним скрытым слоем) и подходящей нелинейной функцией активации, которая приближает $f$ с точностью $\epsilon$ равномерно на $[0, 1]$.

Это фундаментальный результат, утверждающий, что нейронные сети являются \textit{универсальными аппроксиматорами}. Теоретически, даже сеть с одним скрытым слоем может выучить любую зависимость.

Теорема гарантирует \textit{существование} такой сети, но ничего не говорит о том, насколько широким должен быть скрытый слой. На практике ширина слоя может потребоваться экспоненциально большой. Именно поэтому мы используем \textit{глубокие} (многослойные) сети: глубина позволяет строить сложные иерархические признаки более экономно с точки зрения количества параметров.

Итак, нам нужны архитектурные идеи, которые (а) уменьшают число параметров и (б) встраивают априорную структуру данных. Для изображений и спектрограмм таким следующим шагом становятся имеено свёрточные слои.

\section{Сверточные нейронные сети}

Несмотря на универсальность полносвязных слоев, при работе с изображениями они сталкиваются с проблемой потери пространственной структуры (вытягивание изображения в вектор) и колоссальным количеством параметров:
\begin{enumerate}
    \item Если у слоя $n$ входов и $m$ выходов, то параметров $n\times m$, а при больших $n,m$ вычисления становятся дорогими.
    \item Полносвязный слой одинаково смешивает любые координаты входа, хотя в изображениях важна локальность, ведь соседние пиксели явно связаны сильнее, чем дальние.
\end{enumerate}

Решением являются \textit{сверточные слои} (Convolutional Layers), которые учитывают локальную топологию данных.

\subsection{Операция свертки}

Свертка - это операция, применяющая фильтр (ядро) к входному изображению для выделения определенных паттернов. Формально для двумерного случая:
$$Out(x,y) = \sum_{i=-d}^{d} \sum_{j=-d}^{d} In(x+i, y+j) \cdot K(i, j) + b,$$
где $K$ - фильтр (матрица весов) размера $(2d+1) \times (2d+1)$, а $b$ - смещение.

Интуитивно это означает сканирование изображения \textit{окном фильтра}. Один и тот же фильтр скользит по входу и ищет один конкретный паттерн в разных местах: где паттерн совпал - там выход большой.

В классическом компьютерном зрении фильтры (например, Собеля) задавались вручную. В глубоком обучении значения $K(i,j)$ являются обучаемыми параметрами: сеть сама придумывает, какие признаки ей нужно искать.

Техническое замечание. Во многих библиотеках под "convolution" на практике реализуется \textit{корреляция} (без переворота ядра). Для обучения это почти не важно: параметры ядра всё равно подстраиваются.

\subsection{Обобщение на многоканальные изображения}
Реальные изображения (RGB) и скрытые слои имеют множество каналов. Введем определение многоканальной свертки. 

Пусть вход имеет $C$ каналов. Тогда каждый фильтр является трехмерным тензором глубины $C$, а результат вычисляется как сумма сверток по всем каналам:
$$Out(x,y,t) = \sum_{c=1}^{C} \sum_{i=-d}^{d} \sum_{j=-d}^{d} In(x+i, y+j, c) \cdot K(i, j, c, t) + b_t,$$
где $t=1,\dots,T$ — номер выходного фильтра (канала).

Важно: фильтр всегда имеет ту же глубину $C$, что и вход. Количество выходных каналов $T$ определяется количеством фильтров, которое мы задаем при проектировании.

\paragraph{Количество параметров.}
Для сверточного слоя с фильтрами размера $(2d+1) \times (2d+1)$, $C$ входными каналами и $T$ выходными фильтрами число параметров равно:
$$N_{params} = T \times (C \times (2d+1)^2 + 1)$$
Пример: для $C=3$, фильтров $3\times3$ и $T=1000$ получаем всего $28,000$ параметров, что на порядки меньше, чем в полносвязных слоях. Ключевое: число параметров зависит от размера ядра и числа каналов, а не от размера изображения целиком.

\subsection{Свойства сверточных сетей}
Эффективность CNN обусловлена следующими моментами:

\begin{enumerate}
    \item Локальная связность. Каждый выходной нейрон смотрит только на небольшую область входа. Это соответствует природе изображений, где зависимости локальны.
    \item Локальная связность и разделение весов представляют собой форму индуцированного смещения (inductive bias) - встраивания априорных знаний о структуре данных в архитектуру модели
    \item Разделение весов (Shared Weights). Один и тот же фильтр применяется ко всему изображению. Это обеспечивает инвариантность к сдвигу (объект будет найден, где бы он ни находился).
\end{enumerate}

\subsection{Гиперпараметры свёрточных слоёв}
Основные параметры, управляющие геометрией свертки:
\begin{itemize}
    \item Stride - шаг перемещения фильтра. Фактически, как часто мы сдвигаем ядро. Увеличение stride уменьшает размер выхода
    \item Padding (дополнение). Добавление нулей по краям входа (Zero-padding) позволяет сохранить пространственный размер изображения после свертки.
    \item Dilation (прореживание ядра). Пропуск пикселей при применении фильтра для увеличения области обзора без роста числа параметров.
\end{itemize}

Глянем на формулу (размер выхода, 1D/2D по компонентам) для одной пространственной оси:
$$L_{\text{out}}=\left\lfloor\frac{L_{\text{in}}+2P-\text{dil}\cdot(K-1)-1}{S}\right\rfloor+1,$$
 где $K$ - размер ядра, $S$ - stride, $P$ - padding, $\text{dil}$ - dilation.

\subsection{Пулинг (Pooling) и сжатие карт признаков}

Пулинг - это детерминированная операция агрегации по локальному окну, которая уменьшает пространственное разрешение карт признаков. Выделяем два типа пуллинга:

\begin{itemize}
    \item MaxPool: берём максимум в каждом окне $k\times k$ (часто $2\times 2$ со stride $2$).
    \item AvgPool: берём среднее.
\end{itemize}

Зачем нам это в целом? Мы уменьшаем размерность, а значит, вычислений дальше становится меньше. Также делаем представление более устойчивым к малым сдвигам/деформациям. Нам не так важна точная позиция, важен сам факт наличия некого паттерна.

Это один из способов расширять область восприятия следующих слоёв: после нескольких шагов свёртка начинает видеть всё большие фрагменты исходного сигнала.

На практике во многих архитектурах пулинг частично заменяют свёртками с stride $>1$: идея та же - сжать разрешение, - но параметры обучаемые.

\subsection{Проблемы обучения глубоких сетей}

При обучении глубоких сетей с многими слоями часто возникают следующие проблемы: медленная сходимость или отсутствие сходимости и нестабильность градиентов.

Здесь незаменимой становится гипотеза внутреннего ковариационного сдвига (Internal covariate shift) - это предположение, что изменение распределения активаций во внутренних слоях сети в процессе обучения затрудняет обучение последующих слоев, которые оказываются неадаптированными к новым распределениям.

\section{Batch Normalization}

Batch Normalization - это метод нормализации активаций внутренних слоев, вычисляемый по мини-батчам данных во время. Изначально батч-нормализацию мотивировали именно гипотезой внутреннего ковариационного сдвига: если распределения активаций во внутренних слоях постоянно плывут, то последующим слоям тяжелее адаптироваться. Однако важно понимать, что современное (более аккуратное) объяснение эффективности BN не сводится только к этой гипотезе. Нормализация также стабилизирует и сглаживает процесс оптимизации - это мы обсудим детальнее в следующем разделе.

Батч-нормализация масштабирует каждый признак на выходе слоя, вычитая среднее и деля на стандартное отклонение с последующей настройкой параметров $\gamma$ и $\beta$. 

Пусть на некотором слое мы получаем значения активаций $x_{ij}$, где $i=1,\dots,N$ - индекс объекта в мини-батче, а $j=1,\dots,d$ - индекс признака (координаты).
Batch normalization нормализует каждый признак по батчу:
$$
\mu_j=\frac1N\sum_{i=1}^N x_{ij},\qquad
\sigma_j^2=\frac1N\sum_{i=1}^N (x_{ij}-\mu_j)^2,
$$
$$
\hat{x}_{ij}=\frac{x_{ij}-\mu_j}{\sqrt{\sigma_j^2+\varepsilon}},
\qquad y_{ij}=\gamma_j\hat{x}_{ij}+\beta_j,
$$
где $\varepsilon>0$ - малая константа для численной устойчивости, а $\gamma_j,\beta_j$ - обучаемые параметры масштаба и сдвига.

В сверточных сетях batch normalization обычно применяют по каналам: среднее и дисперсия считаются по оси батча и по всем пространственным координатам внутри канала. Это сохраняет согласованность нормализации с топологией данных (внутри канала признаки однотипны).

\subsection{Переосмысление Batch Normalization}

Мы уже ввели понятие Batch Normalization (BN) как метода нормализации активаций, который позволяет обучать глубокие сети. Однако понимание причин его эффективности со временем изменилось.

Первоначальная статья, предложившая BN, утверждала, что метод решает проблему Internal Covariate Shift (ICS). Суть гипотезы была в том, что в процессе обучения параметры предыдущих слоев меняются, из-за чего меняется распределение входов для последующих слоев. Глубоким слоям приходится постоянно подстраиваться под новые статистики входов, что замедляет обучение.

Спустя несколько лет исследователи провели серию экспериментов, которые опровергли эту гипотезу. В одном из экспериментов после каждого слоя BN специально добавляли шум с ненулевым средним и меняющейся дисперсией, искусственно создавая сильный ковариационный сдвиг. Оказалось, что такая зашумленная сеть с BN все равно обучается лучше и быстрее, чем обычная сеть без BN.

Более того, замеры показали, что BN не стабилизирует, а иногда даже увеличивает изменение градиентов в глубоких слоях. Это означает, что борьба со сдвигом распределений не является главной причиной успеха метода. 

Настоящий механизм работы batch normalization заключается в упрощении функционала ошибки - он сглаживает ландшафт функции потерь (Loss Landscape Smoothing). Эксперименты по анализу поведения функции потерь вдоль направления градиента показали:

\begin{itemize}
    \item Без batch normalization наблюдаются значительные колебания значения функции. потерь
    \item С batch normalization функционал становится более гладким и предсказуемым.
    \item Углы между градиентами в соседних точках становятся более стабильными.
\end{itemize}

Это упрощение ландшафта функции ошибки облегчает процесс градиентного спуска и ускоряет обучение.

\subsection{Другие нормализации}

BatchNorm завязан на статистики батча, поэтому в некоторых режимах (малые батчи, переменная длина последовательностей и т.п.) удобнее использовать альтернативы.

Например, Layer Normalization. LayerNorm нормализует внутри одного объекта по координатам признакового вектора:
$$
\mu=\frac1d\sum_{j=1}^d x_j,\qquad
\sigma^2=\frac1d\sum_{j=1}^d (x_j-\mu)^2,
\qquad
y_j=\gamma_j\frac{x_j-\mu}{\sqrt{\sigma^2+\varepsilon}}+\beta_j.
$$
Статистики не зависят от других объектов, поэтому метод стабильно работает при любом размере батча.

Еще есть вариация с Group Normalization. Это такой промежуточный вариант: каналы делятся на группы, и нормализация выполняется внутри каждой группы каналов. Это полезно в сверточных сетях при малых батчах, когда BatchNorm становится шумным.

Отдельно стоит рассмотреть RMSNorm. Эта форма нормализует по корню из среднего квадрата (без вычитания среднего):
$$
\operatorname{RMS}(x)=\sqrt{\frac1d\sum_{j=1}^d x_j^2+\varepsilon},
\qquad
y_j=\gamma_j\frac{x_j}{\operatorname{RMS}(x)}.
$$
На практике это часто используют в архитектурах трансформеров: метод дешевле по вычислениям и достаточно стабилен.

\section{Аугментация данных}

Для задач компьютерного зрения, особенно при работе с ограниченными наборами данных, важную роль играет \textit{аугментация} - техника искусственного расширения выборки за счет преобразований исходных изображений. Типичные аугментации для изображений:

\begin{itemize}
    \item случайные повороты, сдвиги, масштабирование, отражения;
    \item изменения яркости/контраста/насыщенности;
    \item добавление шума, небольшие размытия.
\end{itemize}

Ключевое требование здесь следующее: преобразование не должно менять \emph{семантический класс} объекта. В таком случае аугментация действует как регуляризация: модель вынуждена быть инвариантной к допустимым изменениям входа.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{augmentation.jpg}
\end{figure}

\section{Архитектуры сверточных сетей}


Прорыв в глубоком обучении для компьютерного зрения начался с создания набора данных ImageNet (2011-2012 гг.), содержащего около миллиона изображений, размеченных по 1000 классам. Это позволило проводить масштабные сравнительные исследования.

\subsection{AlexNet}

Первой успешной архитектурой стала \textbf{AlexNet} (названа в честь Алекса Ижевского). Основные характеристики:

\begin{itemize}
    \item Вход: цветные изображения $224 \times 224 \times 3$ (RGB).
    \item Последовательность сверточных слоев с фильтрами $11 \times 11$ и $5 \times 5$.
    \item Максимальное пулинговые слои (max pooling).
    \item В конце - вытягивание в вектор и несколько полносвязных слоев.
    \item Активация ReLU, оптимизация с моментумом.
    \item 60 миллионов параметров.
    \item Ошибка top-5: около 17\% (против 25\% у классических методов).
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{alexnet.jpg}
\end{figure}

Исторический смысл AlexNet - показать, что глубокая CNN при корректной оптимизации и данных большого масштаба радикально превосходит классические подходы того времени.

\subsection{VGG Network}

Архитектура VGG от Visual Geometry Group (Оксфорд) предложила ключевые улучшения - глубину через маленькие фильтры. Это простой, но мощный дизайн:

\begin{itemize}
    \item использовать почти исключительно свертки $3\times 3$;
    \item наращивать глубину сети (в известных конфигурациях - до 16-19 слоев).
\end{itemize}

Малые фильтры позволяют строить много нелинейных преобразований подряд и постепенно увеличивать область восприятия (receptive field), не раздувая параметры слишком быстро. Это повышает выразительную способность модели. Однако обучение таких глубоких архитектур потребовало решения проблем сходимости.

Глубокая архитектура VGG с 19 слоями столкнулась с проблемами обучения: сеть не сходилась или демонстрировала плохое качество. В качестве временного решения использовался костыльный подход. Сначала обучали урезанную версию модели с меньшим числом слоев, затем постепенно добавляли слои, инициализируя их случайно и дообучая сеть. С появлением batch normalization эта проблема была решена, глубокие сети стали обучаться с нуля без дополнительных ухищрений.

Глубокая архитектура VGG достигла ошибки top-5 около 8\% с использованием композиции моделей, что значительно превосходит результаты AlexNet. Ошибка top-5 означает, что в 92\% случаев правильный класс находится среди пяти наиболее вероятных классов по мнению модели.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{vgg.jpg}
\end{figure}

\subsection{ResNet: революция остаточных связей}

При обучении очень глубоких сетей (50+ слоев) возникла проблема деградации. Качество падало даже на обучающей выборке из-за затухания градиентов.

При увеличении глубины сети иногда наблюдается парадокс: качество ухудшается даже на обучающей выборке. Это не похоже на переобучение - это похоже на то, что оптимизация перестала справляться.

Окей, а что если добавть остаточные (residual) связи, которые бы пропускали сигнал в обход нескольких слоев? Это и стало ключевой идеей ResNet. Давайте введем Skip Connection (ту самую остаточную связь):

$$H(x)=F(x)+x,$$

где $x$ - вход блока, $F(x)$ - преобразование (обычно несколько сверток + нелинейности), а $H(x)$ - выход блока.

Вместо того чтобы учить всю функцию целиком, блок учит \emph{поправку} к тождественному отображению. Это облегчает распространение информации и градиента по сети и позволяет эффективно обучать очень глубокие архитектуры.

ResNet позволил обучать сети глубиной в 152 и даже 1000 слоев, снизив ошибку на ImageNet до 4.5\%. Также в ResNet отказались от полносвязных слоев в конце, заменив их на Global Average Pooling, или свертки со страйдом, что резко сократило число параметров.

\begin{thebibliography}{99}

\bibitem{goodfellow2016}
Goodfellow, I., Bengio, Y., Courville, A.
\textit{\href{https://www.deeplearningbook.org/}{Deep Learning}}. // MIT Press, 2016.

\bibitem{d2l}
Zhang, A., Lipton, Z. C., Li, M., Smola, A. J.
\textit{\href{https://d2l.ai/}{Dive into Deep Learning}}. // Online book.

\bibitem{cs231n}
Stanford CS231n:
\textit{\href{https://cs231n.github.io/}{Convolutional Neural Networks for Visual Recognition}}. // Course notes.

\bibitem{alexnet2012}
Krizhevsky, A., Sutskever, I., Hinton, G. E.
\textit{\href{https://proceedings.neurips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf}{ImageNet Classification with Deep Convolutional Neural Networks}}. // NeurIPS, 2012.

\bibitem{resnet}
He, K., Zhang, X., Ren, S., Sun, J.
\textit{\href{https://arxiv.org/abs/1512.03385}{Deep Residual Learning for Image Recognition}}. // CVPR, 2016.

\bibitem{bn}
Ioffe, S., Szegedy, C.
\textit{\href{https://arxiv.org/abs/1502.03167}{Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift}}. // ICML, 2015.

\bibitem{bn_santurkar}
Santurkar, S., Tsipras, D., Ilyas, A., Madry, A.
\textit{\href{https://papers.neurips.cc/paper/7515-how-does-batch-normalization-help-optimization.pdf}{How Does Batch Normalization Help Optimization?}} // NeurIPS, 2018.

\bibitem{layernorm}
Ba, J. L., Kiros, J. R., Hinton, G. E.
\textit{\href{https://arxiv.org/abs/1607.06450}{Layer Normalization}}. // 2016.

\bibitem{groupnorm}
Wu, Y., He, K.
\textit{\href{https://openaccess.thecvf.com/content_ECCV_2018/papers/Yuxin_Wu_Group_Normalization_ECCV_2018_paper.pdf}{Group Normalization}}. // ECCV, 2018.

\bibitem{rmsnorm}
Zhang, B., Sennrich, R.
\textit{\href{https://openreview.net/pdf?id=SygkZ3MTJE}{Root Mean Square Layer Normalization}}. // NeurIPS, 2019.

\end{thebibliography}

\end{document}