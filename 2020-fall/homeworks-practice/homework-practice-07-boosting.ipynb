{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Машинное обучение, ФКН ВШЭ\n",
    "\n",
    "# Практическое задание 7. Бустинг и бэггинг\n",
    "\n",
    "## Общая информация\n",
    "Дата выдачи: 08.12.2020\n",
    "\n",
    "Мягкий дедлайн: 19.12.2020 00:59 MSK\n",
    "\n",
    "Жёсткий дедлайн: 21.12.2020 00:59 MSK\n",
    "\n",
    "## Оценивание и штрафы\n",
    "\n",
    "Каждая из задач имеет определенную «стоимость» (указана в скобках около задачи). Максимально допустимая оценка за работу — 10 баллов.\n",
    "\n",
    "Сдавать задание после указанного срока сдачи нельзя. При выставлении неполного балла за задание в связи с наличием ошибок на усмотрение проверяющего предусмотрена возможность исправить работу на указанных в ответном письме условиях.\n",
    "\n",
    "Задание выполняется самостоятельно. «Похожие» решения считаются плагиатом и все задействованные студенты (в том числе те, у кого списали) не могут получить за него больше 0 баллов (подробнее о плагиате см. на странице курса). Если вы нашли решение какого-то из заданий (или его часть) в открытом источнике, необходимо указать ссылку на этот источник в отдельном блоке в конце вашей работы (скорее всего вы будете не единственным, кто это нашел, поэтому чтобы исключить подозрение в плагиате, необходима ссылка на источник).\n",
    "\n",
    "Неэффективная реализация кода может негативно отразиться на оценке.\n",
    "\n",
    "## Формат сдачи\n",
    "Задания сдаются через систему anytask. Посылка должна содержать:\n",
    "* Ноутбук homework-practice-07-Username.ipynb\n",
    "\n",
    "Username — ваша фамилия и имя на латинице именно в таком порядке"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## О задании\n",
    "\n",
    "В этом задании вам предстоит вручную запрограммировать один из самых мощных алгоритмов машинного обучения — бустинг. Работать мы будем на двух наборах данных: многомерных данных по кредитам с kaggle и синтетических двумерных. В данных с kaggle целевая переменная показывает, вернуло ли кредит физическое лицо:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T11:18:19.115225Z",
     "start_time": "2020-12-07T11:18:17.719538Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, accuracy_score, f1_score, log_loss, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget  -O 'bank_data.csv' -q 'https://www.dropbox.com/s/uy27mctxo0gbuof/bank_data.csv?dl=0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T11:18:19.167210Z",
     "start_time": "2020-12-07T11:18:19.117347Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('bank_data.csv')\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделим на train и test (random_state не меняем)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T11:18:19.175738Z",
     "start_time": "2020-12-07T11:18:19.169377Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Генерируем синтетические данные (seed не меняем)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T11:18:20.422537Z",
     "start_time": "2020-12-07T11:18:19.177840Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "num_obs = 10**5\n",
    "num_thresholds = 50\n",
    "\n",
    "X_synthetic = np.random.normal(scale=3, size=[num_obs, 2])\n",
    "x1_thresholds = np.random.choice(X_synthetic[:, 0], num_thresholds, False)\n",
    "x2_thresholds = np.random.choice(X_synthetic[:, 1], num_thresholds, False)\n",
    "\n",
    "gains = np.random.uniform(-0.4086, 0.5, size=[2 * num_thresholds, 1])\n",
    "x1_thresholds_cond = [X_synthetic[:, 0] >= threshold for threshold in x1_thresholds]\n",
    "x2_thresholds_cond = [X_synthetic[:, 1] >= threshold for threshold in x2_thresholds]\n",
    "\n",
    "noise = np.random.uniform(-0.5, 0.5, size=num_obs)\n",
    "\n",
    "y_synthetic_probits = np.sum(gains[:num_thresholds] * x1_thresholds_cond + gains[num_thresholds:] * x2_thresholds_cond,\n",
    "                     axis=0) + noise\n",
    "y_synthetic = np.sign(y_synthetic_probits)\n",
    "\n",
    "X_train_synthetic, y_train_synthetic = X_synthetic[:int(num_obs * 0.8)], y_synthetic[:int(num_obs * 0.8)]\n",
    "X_test_synthetic, y_test_synthetic = X_synthetic[int(num_obs * 0.8):], y_synthetic[int(num_obs * 0.8):]\n",
    "\n",
    "px.histogram(x = y_synthetic_probits, nbins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Некоторый полезный код для визуализации предсказаний (пригодится позже)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_predicts(model, features, targets, x_lim=[-15.0, 15.0], y_lim=[-15.0, 15.0],\n",
    "                  examples_density=0.01, steps=1000, num_ticks=6, title='', mode='classification'):\n",
    "    '''\n",
    "    Функция для визуализации предсказаний модели на двухмерной плоскости\n",
    "    param model: обученная модель классификации или регрессии для двухмерных объектов\n",
    "    param features: признаки выборки (a.k.a. X)\n",
    "    param targets: целевая переменная выборки (a.k.a y)\n",
    "    param x_lim: пределы для x\n",
    "    param y_lim: пределы для y\n",
    "    param examples_density: доля выборки, которая будет нарисована\n",
    "    param steps: частота разбиения плоскости\n",
    "    param num_ticks: число подписей на графике\n",
    "    param title: заголовок графика\n",
    "    param mode: режим 'classification' - вероятности положительного класса\n",
    "                режим 'regression' - вещественная целевая переменная\n",
    "    '''\n",
    "    \n",
    "    mask = np.random.choice([True, False], size=features.shape[0], \n",
    "                            p=[examples_density, 1.0 - examples_density])\n",
    "    features_x = (features[mask, 0] - x_lim[0]) / (x_lim[1] - x_lim[0]) * steps\n",
    "    features_y = (features[mask, 1] - y_lim[0]) / (y_lim[1] - y_lim[0]) * steps\n",
    "    \n",
    "    xs = np.linspace(x_lim[0], x_lim[1], steps)\n",
    "    ys = np.linspace(y_lim[0], y_lim[1], steps)\n",
    "    \n",
    "    xs, ys = np.meshgrid(xs, ys)\n",
    "    grid = np.stack([xs.flatten(), ys.flatten()], axis=1)\n",
    "    if mode == 'classification':\n",
    "        predicts = model.predict_proba(grid)[:, 1].reshape(steps, steps)\n",
    "        values = (targets[mask] == 1).astype(np.float)\n",
    "    elif mode == 'regression':\n",
    "        predicts = model.predict(grid).reshape(steps, steps)\n",
    "        values = targets[mask]\n",
    "    else:\n",
    "        raise ValueError('Unknown mode')\n",
    "    \n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(predicts, origin='lower')\n",
    "    plt.scatter(features_x, features_y, c=values, edgecolors='white', linewidths=1.5)\n",
    "    plt.colorbar()\n",
    "    \n",
    "    plt.xticks(np.linspace(0, steps, num_ticks), np.linspace(x_lim[0], x_lim[1], num_ticks))\n",
    "    plt.yticks(np.linspace(0, steps, num_ticks), np.linspace(y_lim[0], y_lim[1], num_ticks))\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.title(title)\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. (4 балла) Реализуйте бустинг для задачи бинарной классификации.\n",
    "\n",
    "Поскольку градиентный бустинг обучается через последовательное создание моделей, может получиться так, что оптимальная с точки зрения генерализации модель будет получена на промежуточной итерации. Обычно для контроля такого поведения в методе `fit` передается также валидационная выборка, по которой можно оценивать общее качество модели в процессе обучения (желательно делать это каждую итерацию, но если ваша имплементация слишком медленная или ваше железо не тянет, можно делать это реже). Кроме того, нет смысла обучать действительно глубокую модель на 1000 деревьев и больше, если оптимальный ансамбль получился, к примеру, на 70 итерации и в течение какого-то количества итераций не улучшился - поэтому мы также задействуем early stopping при отсутствии улучшений в течение некоторого числа итераций."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T11:18:20.434619Z",
     "start_time": "2020-12-07T11:18:20.425779Z"
    }
   },
   "outputs": [],
   "source": [
    "class Boosting:\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        base_estimator=DecisionTreeRegressor,\n",
    "        base_estimator_params: dict={'max_features': 0.1},\n",
    "        n_estimators: int=10,\n",
    "        learning_rate: float=0.1,\n",
    "        subsample: float=0.3,\n",
    "        n_jobs: int=8,\n",
    "        random_seed: bool=True,\n",
    "        custom_loss: list or tuple=None,\n",
    "        use_best_model: bool=False,\n",
    "        n_iter_early_stopping: int=None\n",
    "    ):\n",
    "        \n",
    "        # Класс базовой модели\n",
    "        self.base_estimator = base_estimator\n",
    "        # Параметры для инициализации базовой модели\n",
    "        self.base_estimator_params = base_estimator_params\n",
    "        # Число базовых моделей\n",
    "        self.n_estimators = n_estimators\n",
    "        # Длина шага (которая в лекциях обозначалась через eta)\n",
    "        self.learning_rate = learning_rate\n",
    "        # Доля объектов, на которых обучается каждая базовая модель\n",
    "        self.subsample = subsample\n",
    "        # Число используемых ядер для обучения (сколько моделей обучать параллельно)\n",
    "        self.n_jobs = n_jobs\n",
    "        # seed для бутстрапа, если хотим воспроизводимость модели\n",
    "        self.random_seed = random_seed\n",
    "        # Использовать ли при вызове predict и predict_proba лучшее\n",
    "        # с точки зрения валидационной выборки число деревьев в композиции\n",
    "        self.use_best_model = use_best_model\n",
    "        # число итераций, после которых при отсутствии улучшений на валидационной выборке обучение завершается\n",
    "        self.n_iter_early_stopping = n_iter_early_stopping\n",
    "        \n",
    "        # Плейсхолдер для нулевой модели\n",
    "        self.initial_model_pred = None\n",
    "        \n",
    "        # Список для хранения весов при моделях\n",
    "        self.gammas = []\n",
    "        \n",
    "        # Создаем список базовых моделей\n",
    "        self.models = [self.base_model_class(**self.model_params) for _ in range(self.num_models)]\n",
    "        \n",
    "        # Если используем свою функцию потерь, ее нужно передать как список из loss-a и его производной\n",
    "        if custom_loss is not None:\n",
    "            self.loss_fn, self.loss_derivative = custom_loss\n",
    "        else:\n",
    "            self.sigmoid = lambda z: 1 / (1 + np.exp(-z))\n",
    "            self.loss_fn = lambda y, z: -np.log(self.sigmoid(y * z)).mean()\n",
    "            self.loss_derivative = lambda y, z: -y * self.sigmoid(-y * z)\n",
    "        \n",
    "        \n",
    "    def _fit_new_model(self, X: np.ndarray, y: np.ndarray or list, n_model: int):\n",
    "        \"\"\"\n",
    "        Функция для обучения одной базовой модели бустинга\n",
    "        :param X: матрица признаков\n",
    "        :param y: вектор целевой переменной\n",
    "        :param n_model: номер модели, которую хотим обучить\n",
    "        \"\"\"\n",
    "        \n",
    "        # Your code is here ╰( ͡° ͜ʖ ͡° )つ──☆*:\n",
    "        \n",
    "        \n",
    "    def _fit_initial_model(self, X, y):\n",
    "        \"\"\"\n",
    "        Функция для построения нулевой (простой) модели. Не забудьте взять логарифм, потому что сигмоида применяется\n",
    "        уже к сумме предсказаний базовых моделей, а не к каждому предсказанию каждой модели по отдельности.\n",
    "        Подойдёт константная модель, возвращающая самый популярный класс,\n",
    "        но если хотите, можете сделать что-нибудь посложнее.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Your code here ╰( ͡° ͜ʖ ͡° )つ──☆*:\n",
    "    \n",
    "    \n",
    "    def _find_optimal_gamma(self, y: np.ndarray or list, old_predictions: np.ndarray,\n",
    "                            new_predictions: np.ndarray, boundaries: tuple or list=(0.01, 1)):\n",
    "        \"\"\"\n",
    "        Функция для поиска оптимального значения параметра gamma (коэффициент перед новой базовой моделью).\n",
    "        :param y: вектор целевой переменной\n",
    "        :param old_predictions: вектор суммы предсказаний предыдущих моделей (до сигмоиды)\n",
    "        :param new_predictions: вектор суммы предсказаний новой модели (после сигмоиды)\n",
    "        :param boudnaries: в каком диапазоне искать оптимальное значение ɣ (array-like объект из левой и правой границ)\n",
    "        \"\"\"\n",
    "        # Определеяем начальные лосс и оптимальную гамму\n",
    "        loss, optimal_gamma = self.loss_fn(y, old_predictions), 0\n",
    "        # Множество, на котором будем искать оптимальное значение гаммы\n",
    "        gammas = np.linspace(*boundaries, 100)\n",
    "        # Простым перебором ищем оптимальное значение\n",
    "        for gamma in gammas:\n",
    "            predictions = old_predictions + gamma * new_predictions\n",
    "            gamma_loss = self.loss_fn(y, predictions)\n",
    "            if gamma_loss < loss:\n",
    "                optimal_gamma = gamma\n",
    "                loss = gamma_loss\n",
    "        \n",
    "        return optimal_gamma\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y, eval_set=None):\n",
    "        \"\"\"\n",
    "        Функция для обучения всей модели бустинга\n",
    "        :param X: матрица признаков\n",
    "        :param y: вектор целевой переменной\n",
    "        :eval_set: кортеж (X_val, y_val) для контроля процесса обучения или None, если контроль не используется\n",
    "        \"\"\"\n",
    "            \n",
    "        # Your code here ╰( ͡° ͜ʖ ͡° )つ──☆*:\n",
    "        \n",
    "        \n",
    "    def predict(self, X: np.ndarray):\n",
    "        \"\"\"\n",
    "        Функция для предсказания классов обученной моделью бустинга\n",
    "        :param X: матрица признаков\n",
    "        \"\"\"\n",
    "        \n",
    "        # Your code here ╰( ͡° ͜ʖ ͡° )つ──☆*:\n",
    "        \n",
    "    def predict_proba(self, X: np.ndarray):\n",
    "        \"\"\"\n",
    "        Функция для предсказания вероятностей классов обученной моделью бустинга\n",
    "        :param X: матрица признаков\n",
    "        \"\"\"\n",
    "        \n",
    "        # Your code here ╰( ͡° ͜ʖ ͡° )つ──☆*:\n",
    "        \n",
    "    @property\n",
    "    def feature_importances_(self):\n",
    "        \"\"\"\n",
    "        Бонусная часть. Функция для вычисления важностей признаков. Вычисление должно проводиться после обучения модели\n",
    "        и быть доступно атрибутом класса. \n",
    "        \"\"\"\n",
    "        # Your code here ╰( ͡° ͜ʖ ͡° )つ──☆*:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Тест для вашей имплементации. Если класс написан правильно, две следующие ячейки должна отработать без ошибок и относительно быстро (у автора задания 2 и 0.2 секунд соответственно, accuracy 0.911 и 0.879 соответственно). Если у вас получилось качество выше указанного — отлично!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T11:18:22.528176Z",
     "start_time": "2020-12-07T11:18:20.451213Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "boosting = Boosting()\n",
    "boosting.fit(X_train_synthetic, y_train_synthetic)\n",
    "# Без разницы, выдает эта строка классы или вероятности\n",
    "preds = np.round(boosting.predict(X_test_synthetic) > 0.5)\n",
    "assert accuracy_score((y_test_synthetic == 1), np.round(preds)) > 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T11:18:22.733253Z",
     "start_time": "2020-12-07T11:18:22.529999Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "boosting = Boosting()\n",
    "boosting.fit(df_train.select_dtypes(['int64', 'float64']).drop(columns='y').values, df_train.y.values)\n",
    "# Без разницы, выдает эта строка классы или вероятности\n",
    "preds = np.round(boosting.predict(df_test.select_dtypes(['int64', 'float64']).drop(columns='y').values) > 0.5)\n",
    "assert accuracy_score((df_test.y.values == 1), np.round(preds)) > 0.87"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-23T11:06:55.394867Z",
     "start_time": "2020-08-23T11:06:54.568799Z"
    }
   },
   "source": [
    "#### 2. (2 балла) Сравните результаты вашей имплементации бустинга с указанными ниже базовыми моделями на обоих датасетах и ответьте на вопросы. Разумеется, надо измерять качество на тестовых данных. \n",
    "\n",
    "Варианты для базовой модели (разумеется, не надо их программировать самостоятельно, берите нужные классы из sklearn):\n",
    "\n",
    "- Решающее дерево глубины 6\n",
    "- Случайный лес (число деревьев — на ваше усмотрение, только не слишком мало)\n",
    "- Линейная регрессия\n",
    "\n",
    "Вопросы:\n",
    "\n",
    "1) Какая из моделей имеет оптимальное качество? С чем это связано?\n",
    "\n",
    "2) Какая из моделей сильнее переобучается? Есть ли преимущества от использования ранней остановки и обрезания бустинга до лучшей модели?\n",
    "\n",
    "3) Работает ли бустинг над линейными регрессиями лучше, чем одна логистическая регрессия? Как объяснить этот результат?\n",
    "\n",
    "4) Визуализируйте предсказания моделей на синтетическом датасете (для этого можете воспользоваться вспомогательной функцией plot_predicts). Чем отличаются картинки, которые получаются у разных алгоритмов? Сделайте выводы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here ╰( ͡° ͜ʖ ͡° )つ──☆*:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. (2 балла) Мы разобрались с бустингом, теперь интересно посмотреть на совсем дикие комбинации моделей. Сравните результаты следующих моделей на обоих датасетах и ответьте на вопросы. Разумеется, надо измерять качество на тестовых данных.\n",
    "\n",
    "Используйте логистическую регрессию, случайный лес и BaggingClassifier из sklearn.\n",
    "\n",
    "- Случайный лес\n",
    "- Бэггинг на деревьях (поставьте для базовых деревьев min_samples_leaf=1)\n",
    "- Бэггинг на деревьях с обучением каждого дерева на подмножестве признаков (`max_features` около 0.6 в BaggingClassifier)\n",
    "- Бэггинг, у которого базовой моделью является бустинг с большим числом деревьев (> 100)\n",
    "- Бэггинг на логистических регрессиях\n",
    "\n",
    "1) Какая из моделей имеет лучшее качество? С чем это связано?\n",
    "\n",
    "2) Какая из моделей сильнее всего переобучается? Помогает ли бустингу ранняя остановка? \n",
    "\n",
    "3) Исправляет ли бэггинг переобученность бустинга с большим числом деревьев?\n",
    "\n",
    "4) Что лучше: случайный лес или бэггинг на деревьях с сэмплированием признаков?\n",
    "\n",
    "5) Если использовать деревья в качестве базового алгоритма, что лучше — бэггинг или бустинг? С чем это связано?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here ╰( ͡° ͜ʖ ͡° )つ──☆*:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. (2 балла) Сравните на этих данных любую из трёх популярных имплементаций градиентного бустинга (xgboost, lightgbm, catboost) с вашей реализацией. Подберите основные гиперпараметры (число деревьев, длина шага, глубина дерева/число листьев) для обоих методов. Получилось ли у вас победить библиотечные реализации на тестовых данных?\n",
    "\n",
    "__Бонус (1 балл)__: современное развитие методов машинного обучения позволяет автоматизировать и оптимизировать подбор гиперпараметров. Воспользуйтесь для данной задачи одним из фреймворков для такого подбора - например, [hyperopt](https://github.com/hyperopt/hyperopt) или [optuna](https://github.com/optuna/optuna). Сравните также полученные данным методом модели с простым перебором гиперпараметров. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here ╰( ͡° ͜ʖ ͡° )つ──☆*:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. (Бонус, 1 балл) В этом задании мы поговорим об интерпретации моделей.\n",
    "\n",
    "Наша бустинговая модель способна возвращать вероятности для классов. Давайте попробуем оценить, насколько эти вероятности согласованы с реальностью. Для этого мы используем уже знакомый вам метод калибровочных кривых. Постройте калибровочные кривые для бустинговой модели и для логистической регрессии на обоих датасетах. Хорошо ли откалиброваны вероятности бустинга?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here ╰( ͡° ͜ʖ ͡° )つ──☆*:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Бустинг также позволяет рассчитать важность признаков в данных. Для этого существует много подходов, но мы обратимся к самому простому. Поскольку наша базовая модель - это дерево из `sklearn`, мы можем вычислить важность признака отдельно для каждого дерева и усреднить, после этого нормировать значения, чтобы они суммировались в единицу (обратите внимание, что они должны быть неотрицательными - иначе вы что-то сделали не так). Проделайте это, затем нарисуйте столбчатые диаграммы важности признаков для обоих датасетов. На соседних графиках нарисуйте важность признаков для логистической регрессии, для этого используйте модули весов. Сравните графики. Что можно сказать?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here ╰( ͡° ͜ʖ ͡° )つ──☆*:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кстати, чаще всего излишние признаки могут вредить качеству бустинга. Попробуйте отфильтровать на основании диаграммы хвост наименее важных признаков и снова обучить модель (на тех же параметрах!). Стало ли лучше?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here ╰( ͡° ͜ʖ ͡° )つ──☆*:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. (Бонус, 0.01 балла) Готовы ли вы в следующем году пойти ассистентом на этот курс и токсить на набор 19 года во флуде? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "210px",
    "width": "492px"
   },
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
