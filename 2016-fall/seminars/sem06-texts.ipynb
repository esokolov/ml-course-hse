{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Машинное обучение, ФКН ВШЭ\n",
    "\n",
    "# Семинар 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Разреженные матрицы\n",
    "\n",
    "Разреженная матрица — это матрица, большинство элементов которой равны нулю. Такие матрицы возникают во многих областях науки, в том числе и в машинном обучении.\n",
    "\n",
    "Для разреженных матриц можно определить следующие характеристики:\n",
    "- разреженность (sparsity) — доля нулевых элементов матрицы,\n",
    "- плотность (density) — доля ненулевых элементов матрицы, или $1 - \\text(sparsity)$.\n",
    "\n",
    "Для разреженных матриц существуют специальные способы их хранения в памяти компьютера, при которых хранятся только ненулевые значения, тем самым сокращается объём занимаемой памяти. Эти способы реализованы в библиотеке [scipy.sparse](http://docs.scipy.org/doc/scipy/reference/sparse.html). Кроме того, разреженные матрицы поддерживаются большинством реализаций методов машинного обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COOrdinate format\n",
    "\n",
    "[Координатный формат](http://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.coo_matrix.html#scipy.sparse.coo_matrix) задаёт матрицу при помощи троек (индекс строки, индекс столбца, значение элемента), описывающих ненулевые элементы матрицы. Как правило, тройки сортируют по индексу строки, а затем индексу столбца для ускорения работы. \n",
    "\n",
    "Объём занимаемой памяти — $O(n),$ где $n$ — число ненулевых элементов в матрице."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n"
     ]
    }
   ],
   "source": [
    "m = (np.arange(9) + 1).reshape(3,3)\n",
    "print m\n",
    "sparse_m = sp.coo_matrix(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0, 1)\n",
      "(0, 1, 2)\n",
      "(0, 2, 3)\n",
      "(1, 0, 4)\n",
      "(1, 1, 5)\n",
      "(1, 2, 6)\n",
      "(2, 0, 7)\n",
      "(2, 1, 8)\n",
      "(2, 2, 9)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(sparse_m.data)):\n",
    "    print '(%d, %d, %d)' % (sparse_m.row[i], sparse_m.col[i], sparse_m.data[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для матрицы, содержащей нулевые элементы, имеем:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  0.  0.]\n",
      " [ 0.  2.  0.]\n",
      " [ 0.  0.  3.]]\n"
     ]
    }
   ],
   "source": [
    "m = np.eye(3)*np.arange(1,4)\n",
    "print m\n",
    "sparse_m = sp.coo_matrix(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0, 1)\n",
      "(1, 1, 2)\n",
      "(2, 2, 3)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(sparse_m.data)):\n",
    "    print '(%d, %d, %d)' % (sparse_m.row[i], sparse_m.col[i], sparse_m.data[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compressed Sparse Row matrix\n",
    "\n",
    "[CSR формат](http://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html#scipy.sparse.csr_matrix) - разреженная по строчкам матрица. \n",
    "\n",
    "<img src=\"images/arrays.png\">\n",
    "\n",
    "Формат задаёт матрицу при помощи трёх массивов:\n",
    "1. $i$-ый элемент первого массива соответствует $i$-ой строке и содержит индекс некоторого элемента во втором массиве,\n",
    "2. во втором массиве по порядку для каждой строки записаны индексы столбцов ненулевых элементов,\n",
    "3. третий массив имеет такую же длину, как и второй, и содержит значения соответствующих ненулевых элементов.\n",
    "\n",
    "Обозначим описанные массивы $a,b,c$. Для получения элемента матрицы на позиции $(i, j)$ необходимо осуществить следующую последовательность действий:\n",
    "1. Получить значения $a[i]=k_{left}, a[i+1]=k_{right}$.\n",
    "2. Тогда индексы столбцов ненулевых элементов $i$-ой строки будут находиться в \"подмассиве\" $b[k_{left}:k_{right}]$.\n",
    "3. В цикле перебираем элементы подмассива $b[k_{left}:k_{right}]$, пока не встретим элемент, равный $j$.\n",
    "4. Если такой элемент обнаружен на позиции $m$ (в терминах массива $b$), то ответом является значение $c[m]$.\n",
    "5. Иначе ответом является 0.если мы не встретили элемент, равный $j$, то возвращаем $0$.\n",
    "\n",
    "Объём занимаемой памяти — $O(n)$, где $n$ - число ненулевых элементов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n"
     ]
    }
   ],
   "source": [
    "m = (np.arange(9) + 1).reshape(3,3)\n",
    "print m\n",
    "sparse_m = sp.csr_matrix(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a [0 3 6 9]\n",
      "b [0 1 2 0 1 2 0 1 2]\n",
      "c [1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "print 'a', sparse_m.indptr\n",
    "print 'b', sparse_m.indices\n",
    "print 'c', sparse_m.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для матрицы, содержащей нулевые элементы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0]\n",
      " [1 2 0]\n",
      " [1 2 3]]\n"
     ]
    }
   ],
   "source": [
    "m = np.tril(np.arange(1,4))\n",
    "print m\n",
    "sparse_m = sp.csr_matrix(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a [0 1 3 6]\n",
      "b [0 0 1 0 1 2]\n",
      "c [1 1 2 1 2 3]\n"
     ]
    }
   ],
   "source": [
    "print 'a', sparse_m.indptr\n",
    "print 'b', sparse_m.indices\n",
    "print 'c', sparse_m.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compressed Sparse Column matrix\n",
    "\n",
    "[CSC формат](http://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csc_matrix.html#scipy.sparse.csc_matrix) - разреженная по столбцам матрица. \n",
    "\n",
    "Формат CSC задаёт матрицу аналогично формату CSR, но при этом элементы первого массива соответствуют столбцам, а не строкам.\n",
    "\n",
    "Объём занимаемой памяти — $O(n)$, где $n$ - число ненулевых элементов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n"
     ]
    }
   ],
   "source": [
    "m = (np.arange(9) + 1).reshape(3,3)\n",
    "print m\n",
    "sparse_m = sp.csc_matrix(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a [0 3 6 9]\n",
      "b [0 1 2 0 1 2 0 1 2]\n",
      "c [1 4 7 2 5 8 3 6 9]\n"
     ]
    }
   ],
   "source": [
    "print 'a', sparse_m.indptr\n",
    "print 'b', sparse_m.indices\n",
    "print 'c', sparse_m.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0]\n",
      " [1 2 0]\n",
      " [1 2 3]]\n"
     ]
    }
   ],
   "source": [
    "m = np.tril(np.arange(1,4))\n",
    "print m\n",
    "sparse_m = sp.csc_matrix(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a [0 3 5 6]\n",
      "b [0 1 2 1 2 2]\n",
      "c [1 1 1 2 2 3]\n"
     ]
    }
   ],
   "source": [
    "print 'a', sparse_m.indptr\n",
    "print 'b', sparse_m.indices\n",
    "print 'c', sparse_m.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Умножение разреженных матриц\n",
    "\n",
    "Как мы убедились, объём занимаемой памяти практически не отличается для всех вариантов хранения разреженных матриц. В таком случае использование какого из вариантов даёт больше преимуществ? Оказывается, что все три способа кардинально различаются по времени умножения матриц.\n",
    "\n",
    "Для начала вспомним правило умножения матриц:\n",
    "$$C = A\\cdot B$$\n",
    "$$C_{ij} = \\sum_k A_{ik}B_{kj}$$\n",
    "\n",
    "Для нахождения элемента $C_{ij}$ необходимо получить $i$-ую строчку матрицы $A$ и $j$-ый столбец матрицы $B$. Исследуем время выполнения этих операций для каждого из форматов:\n",
    "\n",
    "- **COO.** Стоимость получения строки — $O(n)$. Стоимость получения столбца — $O(n)$. При условии, что тройки отсортированы, время поиска можно сократить, воспользовавшись бинарным поиском.\n",
    "- **CSR.** Стоимость получения строки — $O(1)$. Стоимость получения столбца — $O(n)$.\n",
    "- **CSC.** Стоимость получения строки — $O(n)$. Стоимость получения столбца — $O(1)$.\n",
    "\n",
    "Таким образом, время перемножения матриц будет оптимальным, если матрица $A$ задаётся в формате CSR, а матрица $B$ — в формате CSC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Разреженные матрицы в линейных моделях\n",
    "\n",
    "Рассмотрим задачу линейной регрессии с функционалом качества MSE:\n",
    "\n",
    "$$Q = ||Xw - y||^2 \\rightarrow \\min_{w}.$$\n",
    "\n",
    "Как уже говорилось на предыдущих семинарах, вместо нахождения оптимального значения вектора $w$ используют градиентные методы оптимизации функционала. Запишем формулу его градиента:\n",
    "\n",
    "$$\\frac{\\partial Q}{\\partial w} = 2X^T(Xw - y).$$\n",
    "\n",
    "Заметим, что матрица $X$, заданная в формате CSR, может быть представлена как $X^T$ в формате CSC (действительно, используя те же массивы, мы можем придать им \"симметричный\" смысл).\n",
    "\n",
    "Рассмотрим, как осуществляется умножение разреженной матрицы $A$ на вектор $z$:\n",
    "\n",
    "1) **CSR**\n",
    "$$(Az)_{i} = \\sum_{k}A_{ik}z_k.$$\n",
    "\n",
    "Для матрицы в формале CSR обращение к строчкам матрицы выполняется за $O(1)$, поэтому перемножение выполняется за $O(n)$, где $n$ - кол-во ненулевых элементов матрицы $X$.\n",
    "    \n",
    "2) **CSC** \n",
    "\n",
    "Для матрицы в формате CSC обращение к строчкам матрицы выполняется за $O(n)$. В этом случае умножение будем производить следующим образом:\n",
    "    - Аллоцируем результирующий вектор, который предварительно заполним нулями. \n",
    "    - Обращаемся к $i$-ому столбцу матрицы $A$ и $i$-ому элементу вектора $z$.\n",
    "    - Каждый ненулевой элемент в столбце домножаем на $z_i$ и добавляем результат к соответствующему значению результирующего вектора.\n",
    "    \n",
    "Итого, для умножения разреженной матрицы на вектор получаем следующую асимптотику:\n",
    " - $O(l)$ по памяти;\n",
    " - $O(n)$ по времени.\n",
    "\n",
    "Таким образом, мы описали процедуру умножения разреженной матрицы на вектор, и теперь её можно применить для вычисления градиента в задачах с разреженными матрицами \"объект-признак\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Работа с текстовыми данными\n",
    "\n",
    "Разреженные матрицы имеют место в машинном обучении, в частности, в задачах обработки текстов. \n",
    "\n",
    "Как правило, модели машинного обучения действуют в предположении, что матрица \"объект-признак\" является вещественнозначной, поэтому при работе с текстами сперва для каждого из них необходимо составить его признаковое описание. Для этого широко используются техники векторизации, tf-idf и пр. Рассмотрим их на примере [датасета](https://www.dropbox.com/s/18i7lqac9rr4pnx/banki_responses.json.bz2?dl=0) отзывов о банках.\n",
    "\n",
    "Сперва загрузим данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import bz2\n",
    "import regex\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "201030it [03:00, 1112.37it/s]\n"
     ]
    }
   ],
   "source": [
    "responses = []\n",
    "with bz2.BZ2File('banki_responses.json.bz2', 'r') as thefile:\n",
    "    for row in tqdm(thefile):\n",
    "        resp = json.loads(row)\n",
    "        if not resp['rating_not_checked']:\n",
    "            responses.append(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данные содержат тексты отзывов о банках, некоторую дополнительную информацию, а также оценку банка от 1 до 5. Посмотрим на пример отзыва:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Банк отказывается возвращать депозит после окончания срока действия договора. Предлагает обращаться в ЦО, где в порядке живой очереди можно забирать не более 500$/день в порядке живой очереди. Налицо отвратительное отношение к вкладчикам. Советую всем обращаться с жалобой в ЦБ и роспотребнадзор.\n"
     ]
    }
   ],
   "source": [
    "interesting_responses = filter(lambda r: u'отвратительно' in r['text'], responses)\n",
    "print interesting_responses[0]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Приведём текст отзыва в нижний регистр, а также избавимся от всех символов, кроме кириллицы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "банк отказывается возвращать депозит после окончания срока действия договора  предлагает обращаться в цо  где в порядке живой очереди можно забирать не более      день в порядке живой очереди  налицо отвратительное отношение к вкладчикам  советую всем обращаться с жалобой в цб и роспотребнадзор \n"
     ]
    }
   ],
   "source": [
    "print regex.sub(ur'[^\\p{Cyrillic}]', ' ', interesting_responses[0]['text'].lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сформируем выборку отзывов, предобработав их аналогичным образом, и вектор ответов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "responses = filter(lambda r: r['rating_grade'] is not None, responses)\n",
    "texts = map(lambda r: regex.sub(ur'[^\\p{Cyrillic}]', ' ', r['text'].lower()), responses)\n",
    "ratings = map(lambda r: r['rating_grade'], responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAEACAYAAABGYoqtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4x4V9ACkJgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x109efaf10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "plt.hist(ratings)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Векторизация\n",
    "\n",
    "Самый очевидный способ формирования признакового описания текстов — векторизация. Пусть у нас имеется коллекция текстов $D = \\{d_i\\}_{i=1}^l$ и словарь всех слов, встречающихся в выборке $V = \\{v_j\\}_{j=1}^d.$ В этом случае некоторый текст $d_i$ описывается вектором $(x_{ij})_{j=1}^d,$ где\n",
    "$$x_{ij} = \\sum_{v \\in d_i} [v = v_j].$$\n",
    "\n",
    "Таким образом, текст $d_i$ описывается вектором количества вхождений каждого слова из словаря в данный текст."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(encoding='utf8', min_df=5)\n",
    "_ = vectorizer.fit(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результатом является разреженная матрица."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x72477 sparse matrix of type '<type 'numpy.int64'>'\n",
       "\twith 166 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.transform(texts[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0 166]\n",
      "[ 1314  1842  1843  1846  1917  2672  2804  2977  3187  3262  3831  3862\n",
      "  3863  3864  5324  5339  5354  6100  6119  6509  6652  6982  7024  7698\n",
      "  8477  8522  8523  8895  8995 10656 10685 10692 10712 11245 11941 12082\n",
      " 12408 13565 13619 13620 14154 14940 16054 16082 16226 17106 17752 18197\n",
      " 18584 19698 20584 20936 20937 20967 21257 21520 22045 22141 22376 23163\n",
      " 23241 23460 24523 26125 26148 26763 26840 26889 26890 26891 26892 26937\n",
      " 27673 28023 28658 29869 30470 32743 34358 35198 37304 37363 37610 37687\n",
      " 37853 37944 37998 39345 40300 41288 41765 42128 42282 42681 42717 44133\n",
      " 44559 44881 45301 45343 45578 45691 45881 46121 46309 47329 47868 47918\n",
      " 48642 49433 50104 50979 51905 52268 52397 52656 53267 53269 54002 54776\n",
      " 56797 57464 57715 57749 58252 58422 58625 59542 59570 59573 59592 59704\n",
      " 60046 60655 60657 60965 60996 61003 61794 61924 61928 61988 62255 62672\n",
      " 63116 63684 63686 63926 63996 64379 64480 64572 64607 64613 66307 66654\n",
      " 68632 69825 69845 69858 70320 70516 70814 70891 71338 72055]\n",
      "[1 1 2 1 1 1 1 2 2 1 2 1 1 1 1 1 2 1 1 1 1 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 7 1\n",
      " 1 8 1 2 1 1 2 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 1\n",
      " 2 1 1 2 1 1 1 1 1 1 1 1 1 1 1 7 1 1]\n"
     ]
    }
   ],
   "source": [
    "print vectorizer.transform(texts[:1]).indptr\n",
    "print vectorizer.transform(texts[:1]).indices\n",
    "print vectorizer.transform(texts[:1]).data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "\n",
    "Ещё один способ работы с текстовыми данными — [TF-IDF](https://en.wikipedia.org/wiki/Tf–idf) (**T**erm **F**requency–**I**nverse **D**ocument **F**requency). Рассмотрим коллекцию текстов $D$.  Для каждого уникального слова $t$ из документа $d \\in D$ вычислим следующие величины:\n",
    "\n",
    "1. Term Frequency – количество вхождений слова в отношении к общему числу слов в тексте:\n",
    "$$\\text{tf}(t, d) = \\frac{n_{td}}{\\sum_{t \\in d} n_{td}},$$\n",
    "где $n_{td}$ — количество вхождений слова $t$ в текст $d$.\n",
    "1. Inverse Document Frequency\n",
    "$$\\text{idf}(t, D) = \\log \\frac{\\left| D \\right|}{\\left| \\{d\\in D: t \\in d\\} \\right|},$$\n",
    "где $\\left| \\{d\\in D: t \\in d\\} \\right|$ – количество текстов в коллекции, содержащих слово $t$.\n",
    "\n",
    "Тогда для каждой пары (слово, текст) $(t, d)$ вычислим величину:\n",
    "$$\\text{tf-idf}(t,d, D) = \\text{tf}(t, d)\\cdot \\text{idf}(t, D).$$\n",
    "\n",
    "Отметим, что значение $\\text{tf}(t, d)$ корректируется для часто встречающихся общеупотребимых слов при помощи значения $\\text{idf}(t, D).$\n",
    "\n",
    "Признаковым описанием одного объекта $d \\in D$ будет вектор $\\bigg(\\text{tf-idf}(t,d, D)\\bigg)_{t\\in V}$, где $V$ – словарь всех слов, встречающихся в коллекции $D$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(encoding='utf8', min_df=5)\n",
    "_ = vectorizer.fit(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На выходе получаем разреженную матрицу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x72477 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 166 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.transform(texts[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0 166]\n",
      "[72055 71338 70891 70814 70516 70320 69858 69845 69825 68632 66654 66307\n",
      " 64613 64607 64572 64480 64379 63996 63926 63686 63684 63116 62672 62255\n",
      " 61988 61928 61924 61794 61003 60996 60965 60657 60655 60046 59704 59592\n",
      " 59573 59570 59542 58625 58422 58252 57749 57715 57464 56797 54776 54002\n",
      " 53269 53267 52656 52397 52268 51905 50979 50104 49433 48642 47918 47868\n",
      " 47329 46309 46121 45881 45691 45578 45343 45301 44881 44559 44133 42717\n",
      " 42681 42282 42128 41765 41288 40300 39345 37998 37944 37853 37687 37610\n",
      " 37363 37304 35198 34358 32743 30470 29869 28658 28023 27673 26937 26892\n",
      " 26891 26890 26889 26840 26763 26148 26125 24523 23460 23241 23163 22376\n",
      " 22141 22045 21520 21257 20967 20937 20936 20584 19698 18584 18197 17752\n",
      " 17106 16226 16082 16054 14940 14154 13620 13619 13565 12408 12082 11941\n",
      " 11245 10712 10692 10685 10656  8995  8895  8523  8522  8477  7698  7024\n",
      "  6982  6652  6509  6119  6100  5354  5339  5324  3864  3863  3862  3831\n",
      "  3262  3187  2977  2804  2672  1917  1846  1843  1842  1314]\n",
      "[ 0.05140577  0.12668783  0.10612272  0.08193637  0.0248289   0.05166677\n",
      "  0.03590111  0.05129787  0.08873269  0.09454437  0.05191242  0.12550273\n",
      "  0.03851543  0.04464916  0.04226121  0.08159327  0.07495108  0.198989\n",
      "  0.08752852  0.07894877  0.04408565  0.07335611  0.11606197  0.08795419\n",
      "  0.04919692  0.08525054  0.06207737  0.04037283  0.03823641  0.03951032\n",
      "  0.12441254  0.04981359  0.05392155  0.10501699  0.03027151  0.09732307\n",
      "  0.10167839  0.1182089   0.11151977  0.04671403  0.03496325  0.12389851\n",
      "  0.03260637  0.04823129  0.07020873  0.04905738  0.12389851  0.08545894\n",
      "  0.10514369  0.08125861  0.09431253  0.07115122  0.09173376  0.03312771\n",
      "  0.07837006  0.06430327  0.08082481  0.09402818  0.08202934  0.08125861\n",
      "  0.05805574  0.06574112  0.06701952  0.09538836  0.05215997  0.13847157\n",
      "  0.12036393  0.05724585  0.12389851  0.08267406  0.08738954  0.06379865\n",
      "  0.04645995  0.08528012  0.03329734  0.08304587  0.09908048  0.04857093\n",
      "  0.02926569  0.08335082  0.06869073  0.11151977  0.06790785  0.09595992\n",
      "  0.06309544  0.06822696  0.04850709  0.04202423  0.03946924  0.12340317\n",
      "  0.11430331  0.09932738  0.09018044  0.10115269  0.03931334  0.04298328\n",
      "  0.03559308  0.07772592  0.0588444   0.04010175  0.06610568  0.02152751\n",
      "  0.05476765  0.10358196  0.04779564  0.08799027  0.03861421  0.04282763\n",
      "  0.06173672  0.07965928  0.04371637  0.07803548  0.10592968  0.05952014\n",
      "  0.08545809  0.0597      0.05138855  0.02459478  0.07470369  0.04352039\n",
      "  0.07523134  0.07556094  0.05069349  0.07149121  0.09776012  0.03173102\n",
      "  0.07917796  0.07975763  0.04554608  0.06076308  0.02498441  0.10218103\n",
      "  0.06624403  0.03341379  0.09386037  0.10476707  0.08061298  0.08132489\n",
      "  0.11193253  0.09583082  0.07649131  0.0654067   0.08238374  0.02135754\n",
      "  0.08618662  0.10690796  0.03282988  0.07878639  0.09258752  0.11940833\n",
      "  0.06204507  0.04653723  0.03428121  0.0305839   0.02886322  0.06026069\n",
      "  0.07977738  0.07192711  0.12057397  0.0824317   0.11578113  0.03902659\n",
      "  0.0515867   0.03691948  0.0200837   0.07226592]\n"
     ]
    }
   ],
   "source": [
    "print vectorizer.transform(texts[:1]).indptr\n",
    "print vectorizer.transform(texts[:1]).indices\n",
    "print vectorizer.transform(texts[:1]).data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заметим, что оба метода возвращают вектор длины 72477 (размер нашего словаря)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Лемматизация и стемминг\n",
    "\n",
    "Заметим, что одно и то же слово может встречаться в различных формах (например, \"сотрудник\" и \"сотрудника\"), но описанные выше методы интерпретируют их как различные слова, что делает признаковое описание избыточным. Устранить эту проблему можно при помощи **лемматизации** и **стемминга**.\n",
    "\n",
    "### Стемминг\n",
    "\n",
    "[**Stemming**](https://en.wikipedia.org/wiki/Stemming) –  это процесс нахождения основы слова. В результате применения данной процедуры однокоренные слова, как правило, преобразуются к одинаковому виду.\n",
    "\n",
    "**Примеры стемминга:**\n",
    "\n",
    "| Word        | Stem           |\n",
    "| ----------- |:-------------:|\n",
    "| вагон | вагон |\n",
    "| вагона | вагон |\n",
    "| вагоне | вагон |\n",
    "| вагонов | вагон |\n",
    "| вагоном | вагон |\n",
    "| вагоны | вагон |\n",
    "| важная | важн |\n",
    "| важнее | важн |\n",
    "| важнейшие | важн |\n",
    "| важнейшими | важн |\n",
    "| важничал | важнича |\n",
    "| важно | важн |\n",
    "\n",
    "[Snowball](http://snowball.tartarus.org/) – фрэймворк для написания алгоритмов стемминга. Алгоритмы стемминга отличаются для разных языков и используют знания о конкретном языке – списки окончаний для разных чистей речи, разных склонений и т.д. Пример алгоритма для русского языка – [Russian stemming](http://snowballstem.org/algorithms/russian/stemmer.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "машин обучен\n"
     ]
    }
   ],
   "source": [
    "stemmer = nltk.stem.snowball.RussianStemmer()\n",
    "print stemmer.stem(u'машинное'), stemmer.stem(u'обучение')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:26<00:00, 37.22it/s]\n"
     ]
    }
   ],
   "source": [
    "def stem_text(text, stemmer):\n",
    "    tokens = text.split()\n",
    "    return ' '.join(map(lambda w: stemmer.stem(w), tokens))\n",
    "\n",
    "stemmed_texts = []\n",
    "for t in tqdm(texts[:1000]):\n",
    "    stemmed_texts.append(stem_text(t, stemmer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "для оплаты коммунальных платежей пользуюсь пластиковой картой и сайтом банка  с некоторых пор оплатить отчисления на капремонт стало невозможно  программа требует ввести лицевой счет  это номер квартиры   а после ввода пишет что счет не найден  при внешнем переводе из другого банка сумма возвращается обратно  т к  счет не найден  прошу проверить наличие счета                      и возможность его пополнения \n"
     ]
    }
   ],
   "source": [
    "print texts[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "для оплат коммунальн платеж польз пластиков карт и сайт банк с некотор пор оплат отчислен на капремонт стал невозможн программ треб ввест лицев счет эт номер квартир а посл ввод пишет что счет не найд при внешн перевод из друг банк сумм возвраща обратн т к счет не найд прош провер налич счет и возможн ег пополнен\n"
     ]
    }
   ],
   "source": [
    "print stemmed_texts[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88658"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "К сожалению, стеммер русского языка работает довольно медленно, – 1000 отзывов обрабатываются за 26 секунд, поэтому время обработки всей выборки можно грубо оценить в 40 минут. В связи с этим в рамках семинара мы не будем проводить полную обработку всей выборки, однако вы можете проверить результат работы самостоятельно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Лемматизация\n",
    "\n",
    "[Лемматизация](https://en.wikipedia.org/wiki/Lemmatisation) — процесс приведения слова к его нормальной форме (**лемме**):\n",
    "- для существительных — именительный падеж, единственное число;\n",
    "- для прилагательных — именительный падеж, единственное число, мужской род;\n",
    "- для глаголов, причастий, деепричастий — глагол в инфинитиве."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Классификация\n",
    "\n",
    "Воспользуемся изученными методами обработки текстов для решения задачи классификации отзывов на отзывы с положительной оценкой и отзывы с отрицательной оценкой. Будем считать отзывы с оценками 4-5 положительными, а остальные — отрицательными."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(encoding='utf8', min_df=5)\n",
    "_ = vectorizer.fit(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = vectorizer.transform(texts)\n",
    "Y = (np.array(ratings) > 3).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import ShuffleSplit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC: 0.897, ACC: 0.937\n"
     ]
    }
   ],
   "source": [
    "cv = ShuffleSplit(X.shape[0], n_iter=1, test_size=0.3)\n",
    "for train_ids, test_ids in cv:\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(X[train_ids], Y[train_ids])\n",
    "    preds = lr.predict(X[test_ids])\n",
    "    print 'ROC-AUC: %.3f, ACC: %.3f' % (roc_auc_score(Y[test_ids], preds), accuracy_score(Y[test_ids], preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(encoding='utf8', min_df=5)\n",
    "_ = vectorizer.fit(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = vectorizer.transform(texts)\n",
    "Y = (np.array(ratings) > 3).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC: 0.883, ACC: 0.942\n"
     ]
    }
   ],
   "source": [
    "cv = ShuffleSplit(X.shape[0], n_iter=1, test_size=0.3)\n",
    "for train_ids, test_ids in cv:\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(X[train_ids], Y[train_ids])\n",
    "    preds = lr.predict(X[test_ids])\n",
    "    print 'ROC-AUC: %.3f, ACC: %.3f' % (roc_auc_score(Y[test_ids], preds), accuracy_score(Y[test_ids], preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Важность признаков\n",
    "\n",
    "Как уже упоминалось ранее, веса признаков в линейной модели в случае, если признаки отмасштабированы, характеризуют степень их влияния на значение целевой переменной. В задаче классификации текстов, кроме того, признаки являются хорошо интерпретируемыми, поскольку каждый из них соответствует конкретному слову. Изучим влияние конкретных слов на значение целевой переменной:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "спасибо, 13.79\n",
      "приятно, 10.07\n",
      "благодарность, 9.34\n",
      "быстро, 8.33\n",
      "доволен, 7.07\n",
      "поблагодарить, 6.38\n",
      "очень, 6.02\n",
      "оперативно, 5.62\n",
      "оперативность, 5.41\n",
      "всегда, 5.09\n",
      "очередей, 4.92\n",
      "все, 4.76\n",
      "молодцы, 4.74\n",
      "вопросы, 4.72\n",
      "понравилось, 4.64\n",
      "удобно, 4.53\n",
      "выразить, 4.49\n",
      "нравится, 4.49\n",
      "отметить, 4.30\n",
      "вежливо, 4.14\n",
      "хочу, 4.13\n",
      "большое, 4.10\n",
      "проблем, 4.07\n",
      "пользуюсь, 3.96\n",
      "ткс, 3.94\n",
      "сразу, 3.91\n",
      "оценку, 3.91\n",
      "надеюсь, 3.87\n",
      "банков, 3.87\n",
      "...\n",
      "нормально, -2.53\n",
      "наконец, -2.58\n",
      "говорят, -2.59\n",
      "ничего, -2.59\n",
      "могут, -2.64\n",
      "завтра, -2.66\n",
      "вопрос, -2.66\n",
      "звонки, -2.67\n",
      "что, -2.72\n",
      "пор, -2.72\n",
      "клиентов, -2.82\n",
      "часа, -2.91\n",
      "нельзя, -2.98\n",
      "звоню, -3.01\n",
      "ладно, -3.01\n",
      "невозможно, -3.02\n",
      "сказали, -3.03\n",
      "опять, -3.04\n",
      "видимо, -3.06\n",
      "якобы, -3.07\n",
      "должен, -3.15\n",
      "ответа, -3.21\n",
      "сегодня, -3.35\n",
      "никто, -3.44\n",
      "нет, -3.54\n",
      "зачем, -3.98\n",
      "вы, -3.99\n",
      "почему, -4.13\n",
      "ответ, -4.66\n"
     ]
    }
   ],
   "source": [
    "f_weights = zip(vectorizer.get_feature_names(), lr.coef_[0])\n",
    "f_weights = sorted(f_weights, key=lambda i: i[1])\n",
    "for i in range(1,30):\n",
    "    print '%s, %.2f' % f_weights[-i]\n",
    "    \n",
    "print '...'\n",
    "for i in reversed(range(1,30)):\n",
    "    print '%s, %.2f' % f_weights[i]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
