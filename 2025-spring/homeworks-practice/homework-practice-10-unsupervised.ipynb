{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euigdTiKg7DR"
      },
      "source": [
        "# Машинное обучение, ФКН ВШЭ\n",
        "\n",
        "# Практическое задание 10. Обучение без учителя.\n",
        "\n",
        "## Общая информация\n",
        "Дата выдачи: 26.03.2025\n",
        "\n",
        "Мягкий дедлайн: 13.04.2025 23:59 MSK\n",
        "\n",
        "Жёсткий дедлайн: 17.04.2025 23:59 MSK\n",
        "\n",
        "## Оценивание и штрафы\n",
        "\n",
        "Каждая из задач имеет определенную «стоимость» (указана в скобках около задачи).\n",
        "\n",
        "Сдавать задание после указанного срока сдачи нельзя.\n",
        "\n",
        "Задание выполняется самостоятельно. «Похожие» решения считаются плагиатом и все задействованные студенты (в том числе те, у кого списали) не могут получить за него больше 0 баллов (подробнее о плагиате см. на странице курса). Если вы нашли решение какого-то из заданий (или его часть) в открытом источнике, необходимо указать ссылку на этот источник в отдельном блоке в конце вашей работы (скорее всего вы будете не единственным, кто это нашел, поэтому чтобы исключить подозрение в плагиате, необходима ссылка на источник).\n",
        "\n",
        "Неэффективная реализация кода может негативно отразиться на оценке.\n",
        "\n",
        "## Формат сдачи\n",
        "Задания сдаются через систему anytask. Посылка должна содержать:\n",
        "* Ноутбук homework-practice-10-Username.ipynb\n",
        "\n",
        "Username — ваша фамилия на латинице\n",
        "\n",
        "## О задании\n",
        "\n",
        "В этом задании мы посмотрим на несколько алгоритмов кластеризации и применим их к географическим и текстовым данным. Также мы подробно остановимся на тематическом моделировании текстов, задаче обучения представлений и в каком-то смысле поработаем с semi-supervised learning.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95cdXa4PX8ks"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(0xFFFFFFF)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXRa3lV19H6P"
      },
      "source": [
        "**Задание 0 (1e-100 балла)**. Опишите свои ощущения от домашки по ЕМ-алгоритму (можно картинкой или мемом)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7cEgKmw9KHy"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE (ノಠ益ಠ)ノ彡┻━┻"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uK5GdpMuppA3"
      },
      "source": [
        "## Часть 1. Кластеризация автобусных остановок"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVdORZxZiZTS"
      },
      "source": [
        "В этом задании мы сравним разные алгоритмы кластеризации для данных об автобусных остановках Москвы.\n",
        "\n",
        "**Задание 1.1 (1 балл).** Реализуйте алгоритм спектральной кластеризации, который упоминался на лекции. Для этого разберитесь с кодом шаблона, данного ниже, и допишите недостающую функцию. Напомним, что для графа с матрицей смежности $W = \\{w_{ij}\\}_{i, j = 1 \\dots \\ell}$ лапласиан определяется как:\n",
        "\n",
        "$$\n",
        "L = D - W,\n",
        "$$\n",
        "\n",
        "где $D = \\text{diag}(d_1, ..., d_{\\ell}), d_i = \\sum_{j=1}^{\\ell} w_{ij}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8uVuBJPBixGT"
      },
      "outputs": [],
      "source": [
        "from sklearn.base import ClusterMixin\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "\n",
        "class GraphClustering(ClusterMixin):\n",
        "    def __init__(self, n_clusters=8, n_components=None, **kwargs):\n",
        "        '''\n",
        "        Spectral clustering algorithm\n",
        "        param n_clusters: number of clusters to form\n",
        "        param n_components: number of eigenvectors to use\n",
        "        '''\n",
        "\n",
        "        if n_components is None:\n",
        "            n_components = n_clusters\n",
        "\n",
        "        self.n_components = n_components\n",
        "        self.kmeans = KMeans(n_clusters=n_clusters, **kwargs)\n",
        "\n",
        "    def fit_predict(self, X, y=None):\n",
        "        '''\n",
        "        Perform spectral clustering from graph adjacency matrix\n",
        "        and return vertex labels.\n",
        "        param X: (n_samples, n_samples) - graph adjacency matrix\n",
        "        return: (n_samples, ) - vertex labels\n",
        "        '''\n",
        "\n",
        "        eigenvectors = self._generate_eigenvectors(X)\n",
        "        labels = self.kmeans.fit_predict(eigenvectors[:, 1:])\n",
        "        return labels\n",
        "\n",
        "    def _generate_eigenvectors(self, X):\n",
        "        '''\n",
        "        Compute eigenvectors for spectral clustering\n",
        "        param X: (n_samples, n_samples) - graph adjacency matrix\n",
        "        return: (n_samples, n_components) - eigenvectors\n",
        "        '''\n",
        "\n",
        "        # YOUR CODE HERE ‿︵‿︵ヽ(°□° )ノ︵‿︵‿\n",
        "        raise NotImplementedError"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOFgh62FoKPB"
      },
      "source": [
        "Перед тем, как переходить к следующему заданию, протестируйте свое решение."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HvMVPvHYoPw_"
      },
      "outputs": [],
      "source": [
        "n_blocks, n_vertices = 10, 1000\n",
        "block_vertices = n_vertices // n_blocks\n",
        "\n",
        "X = np.zeros((n_vertices, n_vertices))\n",
        "for i in range(0, n_vertices, block_vertices):\n",
        "    X[i:i + block_vertices, i:i + block_vertices] = np.sqrt(i + 1)\n",
        "\n",
        "graph_clustering = GraphClustering(n_clusters=n_blocks)\n",
        "labels = graph_clustering.fit_predict(X)\n",
        "\n",
        "true_labels = np.zeros(n_vertices, dtype=np.int32)\n",
        "for i in range(0, n_vertices, block_vertices):\n",
        "    true_labels[i:i + block_vertices] = labels[i]\n",
        "\n",
        "assert labels.shape == (n_vertices, )\n",
        "assert np.all(np.bincount(labels) == np.full(n_blocks, block_vertices))\n",
        "assert np.all(labels == true_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ueXlg87of70"
      },
      "source": [
        "Теперь можем приступить к работе с реальными данными. Скачайте файл с данными об остановках общественного транспорта **в формате .xlsx** по [ссылке](https://data.mos.ru/opendata/download/60464/1/201) (так гарантированно не возникнет проблем с парсингом файла) и загрузите таблицу в ноутбук. Альтернативная ссылка: [отсюда](https://disk.yandex.ru/i/J0HlcDXd_KyIeg). Для удобства визуализации мы будем работать только с остановками в ЦАО."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6WWxngQO8Jt3"
      },
      "outputs": [],
      "source": [
        "data = pd.read_excel('transport.xlsx')\n",
        "data = data[data.AdmArea_en == \"Czentral`ny'j administrativny'j okrug\"]\n",
        "data = data.reset_index()\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEJdUfnY9qFj"
      },
      "source": [
        "Воспользуемся библиотекой `folium` для визуализации данных."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KosUV23W9xgn"
      },
      "outputs": [],
      "source": [
        "import folium\n",
        "\n",
        "map = folium.Map([55.75215, 37.61819], zoom_start=12)\n",
        "for id, row in data.iterrows():\n",
        "    folium.Circle([row.Latitude_WGS84_en, row.Longitude_WGS84_en],\n",
        "                  radius=10).add_to(map)\n",
        "map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4oB0OQk-MLP"
      },
      "source": [
        "**Задание 1.2 (1 балл).** Попробуем построить граф, в котором вершинами будут остановки. Как вы уже могли заметить, для каждой остановки указаны номера маршрутов, проходящих через неё. Логично соединить ребрами соседние остановки каждого маршрута. Однако мы не знаем, в каком порядке автобусы объезжают остановки. Но мы можем применить эвристический алгоритм, который восстановит нам порядок маршрутов:\n",
        "\n",
        "* Для каждого маршрута выделим список всех остановок, через которые он проходит.\n",
        "* Выберем начальную остановку маршрута как точку, наиболее удаленную от всех остальных остановок этого маршрута.\n",
        "* Каждую следующую точку маршрута будем выбирать как самую близкую из оставшихся точек (не включенных в маршрут ранее).\n",
        "\n",
        "Фактически, у нас получается жадное решение задачи коммивояжера. Когда мы отсортировали маршруты, можем построить по ним граф. Будем строить его по таким правилам:\n",
        "\n",
        "* Между двумя остановками будет ребро, если они являются соседними хотя бы на одном маршруте. Вес ребра равен числу маршрутов, на которых остановки являются соседними.\n",
        "* В графе не будет петель (то есть у матрицы смежности будет нулевая диагональ).\n",
        "\n",
        "Реализуйте предложенный способ построения графа. Для этого рекомендуется воспользоваться шаблонами, приведенными ниже."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJTwEzjitM0c"
      },
      "outputs": [],
      "source": [
        "def get_routes(data):\n",
        "    '''\n",
        "    Accumulate routes from raw data\n",
        "    param data: pd.DataFrame - public transport stops data\n",
        "    return: dict - unsorted stops ids for each route,\n",
        "                   e.g. routes['A1'] = [356, 641, 190]\n",
        "    '''\n",
        "\n",
        "    # YOUR CODE HERE ‿︵‿︵ヽ(°□° )ノ︵‿︵‿\n",
        "    raise NotImplementedError\n",
        "\n",
        "\n",
        "def sort_routes(data, routes):\n",
        "    '''\n",
        "    Sort routes according to the proposed algorithm\n",
        "    param data: pd.DataFrame - public transport stops data\n",
        "    param routes: dict - unsorted stops ids for each route\n",
        "    return: dict - sorted stops ids for each route\n",
        "    '''\n",
        "\n",
        "    # YOUR CODE HERE ‿︵‿︵ヽ(°□° )ノ︵‿︵‿\n",
        "    raise NotImplementedError\n",
        "\n",
        "\n",
        "def get_adjacency_matrix(data, sorted_routes):\n",
        "    '''\n",
        "    Compute adjacency matrix for sorted routes\n",
        "    param data: pd.DataFrame - public transport stops data\n",
        "    param sorted_routes: dict - sorted stops ids for each route\n",
        "    return: (n_samples, n_samples) - graph adjacency matrix\n",
        "    '''\n",
        "\n",
        "    # YOUR CODE HERE ‿︵‿︵ヽ(°□° )ノ︵‿︵‿\n",
        "    raise NotImplementedError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l2fMkb9VwBIr"
      },
      "outputs": [],
      "source": [
        "routes = get_routes(data)\n",
        "sorted_routes = sort_routes(data, routes)\n",
        "adjacency_matrix = get_adjacency_matrix(data, sorted_routes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ot-EUmMKwEx1"
      },
      "source": [
        "Проверим, что маршруты получились адекватными. Для этого нарисуем их на карте."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P1d67q5oxPHs"
      },
      "outputs": [],
      "source": [
        "map = folium.Map([55.75215, 37.61819], zoom_start=12)\n",
        "for route_id in np.random.choice(list(sorted_routes.keys()), size=5):\n",
        "    coords = data.loc[\n",
        "        sorted_routes[route_id],\n",
        "        ['Latitude_WGS84_en', 'Longitude_WGS84_en']\n",
        "    ].values.tolist()\n",
        "    folium.vector_layers.PolyLine(coords).add_to(map)\n",
        "\n",
        "map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c03XzwlS_bmU"
      },
      "source": [
        "**Задание 1.3 (0 баллов)**. Реализуйте функцию `draw_clustered_map`, которая рисует карту центра Москвы с кластерами остановок, раскрашенными в разные цвета."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fwA4RxiC65Xn"
      },
      "outputs": [],
      "source": [
        "def draw_clustered_map(data, labels):\n",
        "    '''\n",
        "    Create map with coloured clusters\n",
        "    param data: pd.DataFrame - public transport stops data\n",
        "    param labels: (n_samples, ) - cluster labels for each stop\n",
        "    return: folium.Map - map with coloured clusters\n",
        "    '''\n",
        "\n",
        "    # YOUR CODE HERE ‿︵‿︵ヽ(°□° )ノ︵‿︵‿\n",
        "    raise NotImplementedError"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYnX3Ws__ga0"
      },
      "source": [
        "**Задание 1.4 (1.5 балла)**. Примените алгоритмы кластеризации и подберите гиперпараметры так, чтобы кластеры получились осмысленными (нет такого, что все принадлежит одному кластеру или большая часть точек это шум):\n",
        "- `sklearn.clustering.KMeans` - `n_clusters`\n",
        "- `sklearn.clustering.DBSCAN` - `eps`\n",
        "- ваш `SpectralClustering` - `n_clusters` и `n_components`\n",
        "\n",
        "Для ряда алгоритмов, в частности DBSCAN, подобрать оптимальные параметры может быть ой, как непросто. Чтобы помочь сохранить нервные клетки рекомендуется пользоваться метриками Intrinsic Evaluation для кластеризации. Для каждого алгоритма скорее всего придется брать разные метрики, а может быть оптимизировать сразу несколько.\n",
        "\n",
        "Ваша задача - выбрать подходящую метрику, можно взять что-то из списка ниже, можно спросить у гугла. Применить GridSearch, `optuna` или `hyperopt` и желательно завернуть отбор параметров в функцию, пригодится далее\n",
        "\n",
        "1. $\n",
        "   \\text{Calinski-Harabasz Index} \\in [0, \\inf] = \\frac{\\sum_{i=1}^k BCSS_i/(k-1)}{\\sum_{i=1}^k WCSS_i/(n-k)} = \\frac{\\sum_{i=1}^k n_i \\| \\mathbf{c}_i - \\mathbf{c} \\|^2/(k-1)}{\\sum_{i=1}^k \\sum_{\\mathbf{x} \\in C_i} \\|\\mathbf{x} - \\mathbf{c}_i\\|^2/(n-k)} \\rightarrow max\n",
        "   $\n",
        "   - Отношение 1) расстояния от центра кластера до центра всех данных к 2) расстоянию от центра кластера до точки\n",
        "   - Чем однороднее и дальше друг от друга кластеры, тем лучше\n",
        "2. $\n",
        "   \\text{Silhouette Score} \\in [0, 1] = \\sum_i \\frac{b_i - a_i}{\\max(a_i, b_i)} \\rightarrow max\n",
        "   $\n",
        "   - $a_i$ - среднее расстояние до точек внутри своего кластера\n",
        "   - $b_i$ - среднее расстояние до точек из ближайшего кластера\n",
        "   - Если в своем кластере точки близко - кластер плотный, идеальный вариант, скор равен 1, если свой кластер совпадает с соседним - кластеры неразделимы, скор равен нулю\n",
        "   - Чем однороднее и разделимее кластеры, тем лучше\n",
        "3. $\\text{Davies-Bouldin Index} \\in [0, inf] = \\frac{1}{k} \\sum_{i=1}^{k} \\max_{j \\neq i} \\left( \\frac{WCSS_i + WCSS_j}{d(c_i, c_j)} \\right) \\rightarrow min $\n",
        "   - Отношение внутрикластерных расстояний к межкластерному расстоянию между двумя центрами\n",
        "   - Если кластеры далеко друг от друга, но при этом однородные, скор наименьший\n",
        "3. $\\text{Elbow Method}$ - последовательное сравнение $WCSS$ из пункта 1 для разного числа параметров. Если уменьшение меньше определенного порога - останавливаемся, считаем это значение оптимальным"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RliT89AL8vA9"
      },
      "outputs": [],
      "source": [
        "def clustering_objective():\n",
        "    # YOUR CODE HERE ‿︵‿︵ヽ(°□° )ノ︵‿︵‿"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Визуализируйте результат кластеризации с помощью функции `draw_clustered_map`."
      ],
      "metadata": {
        "id": "FCre6H3t7uLC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE ‿︵‿︵ヽ(°□° )ノ︵‿︵‿"
      ],
      "metadata": {
        "id": "ARPZ45Gb8Luk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIZIvcM8BF_I"
      },
      "source": [
        "**Вопрос:** Чем отличаются разбиения на кластеры, получаемые разными алгоритмами? Какие плюсы и минусы есть у каждого алгоритма? Какой алгоритм кажется вам наиболее подходящим для кластеризации остановок?\n",
        "\n",
        "**Ответ:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pr3WXyPBptaN"
      },
      "source": [
        "## Часть 2. Тематическое моделирование текстов"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lh2yJqxhigf4"
      },
      "source": [
        "В этой части мы познакомимся с одной из самых популярных задач обучения без учителя &mdash; с задачей тематического моделирования текстов. Допустим, нам доступна некоторая коллекция документов без разметки, и мы хотим автоматически выделить несколько тем, которые встречаются в документах, а также присвоить каждому документу одну (или несколько) тем. Фактически, мы будем решать задачу, похожую на кластеризацию текстов: отличие в том, что нас будет интересовать не только разбиение текстов на группы, но и выделение ключевых слов, определяющих каждую тему.\n",
        "\n",
        "Мы будем работать с новостными статьями BBC за 2004-2005 годы. Скачайте данные по [ссылке](https://www.kaggle.com/hgultekin/bbcnewsarchive)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMefb5XsixgH"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('bbc-news-data.csv', sep='\\t')\n",
        "data.sample(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMiigCjtY0yh"
      },
      "source": [
        "Как вы могли заметить, данные уже содержат разметку по тематике (колонка category). В этой части мы забудем, что она есть, и будем работать только с текстовыми данными. Проведем предобработку текста, состоящую из следующих пунктов:\n",
        "\n",
        "* Объединим заголовок и содержание статьи в одно поле.\n",
        "* Приведем текст к нижнему регистру, разобьем его на токены.\n",
        "* Оставим только буквенные слова (удалив, таким образом, пунктуацию и числа).\n",
        "* Применим лемматизацию.\n",
        "* Удалим стоп-слова.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XpSY0M7kbIdl"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sk3yw5aNbRgi"
      },
      "outputs": [],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('english') + ['ha', 'wa', 'say', 'said'])\n",
        "lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nbMsHeS2bV2l"
      },
      "outputs": [],
      "source": [
        "def preprocess(text):\n",
        "    text = list(filter(str.isalpha, word_tokenize(text.lower())))\n",
        "    text = list(lemmatizer.lemmatize(word) for word in text)\n",
        "    text = list(word for word in text if word not in stop_words)\n",
        "    return ' '.join(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xhICemtQbg0o"
      },
      "outputs": [],
      "source": [
        "data['raw_text'] = data.apply(lambda row: row.title + row.content, axis=1)\n",
        "data['text'] = data.apply(lambda row: preprocess(row.raw_text), axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0O5ygZPcTkl"
      },
      "source": [
        "Для визуализации частот слов в текстах мы будем использовать [облака тегов](https://en.wikipedia.org/wiki/Tag_cloud)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jRo7Q2bXczEZ"
      },
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud\n",
        "\n",
        "def draw_wordcloud(texts, max_words=1000, width=1000, height=500):\n",
        "    wordcloud = WordCloud(background_color='white', max_words=max_words,\n",
        "                          width=width, height=height)\n",
        "\n",
        "    joint_texts = ' '.join(list(texts))\n",
        "    wordcloud.generate(joint_texts)\n",
        "    return wordcloud.to_image()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTnO8HwGdSvA"
      },
      "outputs": [],
      "source": [
        "draw_wordcloud(data.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MoyLGx-dcjF"
      },
      "source": [
        "**Задание 2.1 (1 балл).** Обучите алгоритм K-Means на tf-idf представлениях текстов. При обучении tf-idf векторайзера рекомендуется отбрасывать редко встречающиеся слова, а также воздержитесь от использования N-грамм. Возьмите не очень большое число кластеров, чтобы было удобно интерпретировать получившиеся темы (например, `n_clusters` = 8). Постройте облака тегов для текстов из разных кластеров."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cr65aXYRU7f2"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE ‿︵‿︵ヽ(°□° )ノ︵‿︵‿"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Получились ли темы интерпретируемыми? Попробуйте озаглавить каждую тему.\n",
        "\n",
        "**Ответ:**"
      ],
      "metadata": {
        "id": "wQ078Gpa8SFG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b41ggvxRU_Oc"
      },
      "source": [
        "**Задание 2.2 (0.5 балла).** Попробуем другой способ выделить ключевые слова для каждой темы. Помимо непосредственного разбиения объектов алгоритм K-Means получает центр каждого кластера. Попробуйте взять центры кластеров и посмотреть на слова, для которых значения соответствующих им признаков максимальны."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EYm-9i2bX7tJ"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE ‿︵‿︵ヽ(°□° )ノ︵‿︵‿"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Согласуются ли полученные слова с облаками тегов из прошлого задания?\n",
        "\n",
        "**Ответ:**"
      ],
      "metadata": {
        "id": "d7CwynaP8UpJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjiogHYuYB0p"
      },
      "source": [
        "**Задание 2.3 (1.5 балла).** В первой части мы сравнили три разных алгоритма кластеризации на географических данных. Проделаем то же самое для текстовых данных (в качестве признакого описания снова используем tf-idf). Получите три разбиения на кластеры с помощью алгоритмов K-Means, DBSCAN и спектральной кластеризации (на этот раз воспользуйтесь реализацией из `sklearn`). Для K-Means и спектральной кластеризации возьмите одинаковое небольшое число кластеров, подберите параметр `eps` метода DBSCAN так, чтобы получить приблизительно такое же число кластеров (подумайте, как это можно сделать)\n",
        "\n",
        "Далее, обучите двухмерные t-SNE представления над tf-idf признаками текстов. Визуализируйте эти представления для каждого алгоритма, раскрасив каждый кластер своим цветом. Лучше всего расположить визуализации на одном графике на трех разных сабплотах. Не забудьте, что DBSCAN помечает некоторые точки как шумовые (можно раскрасить их в отдельный цвет)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B0fA0yEOYg_e"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE ‿︵‿︵ヽ(°□° )ノ︵‿︵‿"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZo8daJrcyTR"
      },
      "source": [
        "Прокомментируйте получившиеся результаты. Какой баланс кластеров получился у разных методов? Соотносятся ли визуализации для текстов с визуализациями для географических данных?\n",
        "\n",
        "**Ответ:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7zArHv6dsVg"
      },
      "source": [
        "**Задание 2.4 (1.5 балла).** Обучите модель латентного размещения Дирихле. Не забудьте, что она работает с мешком слов, а не с tf-idf признаками. Придумайте, как превратить распределение тем для текста в номер его кластера. Возьмите параметр `n_components` в 2-3 раза больше, чем число кластеров для K-Means."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bGw_tnc_dwgT"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE ‿︵‿︵ヽ(°□° )ノ︵‿︵‿"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Получились ли темы более узкими от такого нововведения? Постройте облака тегов для нескольких наиболее удачных тем.\n",
        "\n",
        "**Ответ:**"
      ],
      "metadata": {
        "id": "HH52qDnl8cKd"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfWNOqFZii3J"
      },
      "source": [
        "## Часть 3. Transfer learning и Self-Supervised Learning для задачи классификации текстов"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4clP98BXGLax"
      },
      "source": [
        "**Задание 3.1 (0.5 балла).** Вспомним, что у нас есть разметка для тематик статей. Попробуем обучить классификатор поверх unsupervised-представлений для текстов. Рассмотрите три модели:\n",
        "\n",
        "* Логистическая регрессия на tf-idf признаках\n",
        "* K-Means на tf-idf признаках + логистическая регрессия на расстояниях до центров кластеров\n",
        "* Латентное размещение Дирихле + логистическая регрессия на вероятностях тем\n",
        "\n",
        "Разделите выборку на обучающую и тестовую, замерьте accuracy на обоих выборках для всех трех моделей. Параметры всех моделей возьмите равными значениям по умолчанию."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7KiHqfkhO_R"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE ‿︵‿︵ヽ(°□° )ノ︵‿︵‿"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsbJHnR8gzHX"
      },
      "source": [
        "У какой модели получилось лучшее качество? С чем это связано?\n",
        "\n",
        "**Ответ:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18vaNs6XGScR"
      },
      "source": [
        "**Задание 3.2 (1.5 балла).** Теперь просимулируем ситуацию слабой разметки, которая часто встречается в реальных данных. Разделим обучающую выборку в пропорции 5:65:30. Будем называть части, соответственно, размеченный трейн, неразмеченный трейн и валидация.\n",
        "\n",
        "Все unsupervised-алгоритмы (векторайзеры и алгоритмы кластеризации) запускайте на всем трейне целиком (размеченном и неразмеченном, суммарно 70%), а итоговый классификатор обучайте только на размеченном трейне (5%). Подберите гиперпараметры моделей по качеству на валидации (30%), а затем оцените качество на тестовой выборке (которая осталась от прошлого задания). Не скромничайте при подборе числа кластеров, сейчас нас интересует не интерпретируемое разбиение выборки, а итоговое качество классификации."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n45ZJCDAGTQG"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE ‿︵‿︵ヽ(°□° )ノ︵‿︵‿"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMP7BslkkYAl"
      },
      "source": [
        "Как изменились результаты по сравнению с обучением на полной разметке? Сделайте выводы.\n",
        "\n",
        "**Ответ:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTL9sgADKKah"
      },
      "source": [
        "## Бонус"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkAs4eBx8CXr"
      },
      "source": [
        "**Задание 4 (0.5 балла)**. Разберитесь с semi-supervised методами, которые реализованы в `sklearn` и примените их к заданию 3.2. Получилось ли добиться лучшего качества? Сделайте выводы."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OHTs3_ssKVG7"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE ‿︵‿︵ヽ(°□° )ノ︵‿︵‿"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOI6tcLoKVoS"
      },
      "source": [
        "**Задание 5 (1 балл)**. На занятиях мы обсуждали, что метрика [BCubed](https://www.researchgate.net/profile/Julio-Gonzalo-2/publication/225548032_Amigo_E_Gonzalo_J_Artiles_J_et_alA_comparison_of_extrinsic_clustering_evaluation_metrics_based_on_formal_constraints_Inform_Retriev_12461-486/links/0c96052138dbb99740000000/Amigo-E-Gonzalo-J-Artiles-J-et-alA-comparison-of-extrinsic-clustering-evaluation-metrics-based-on-formal-constraints-Inform-Retriev-12461-486.pdf) хорошо подходит для сравнения алгоритмов кластеризации, если нам известно настоящее разделение на кластеры (gold standard). Реализуйте подсчет метрики BCubed и сравните несколько алгоритмов кластеризации на текстовых данных из основного задания. В качестве gold standard используйте разметку category."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQtZe7Ty847i"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE ‿︵‿︵ヽ(°□° )ノ︵‿︵‿"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9oePmfZKc7s"
      },
      "source": [
        "**Задание 6 (2 баллa)**. Спектральная кластеризация, по сути, является обычной кластеризацией KMeans поверх эмбеддингов объектов, которые получаются из лапласиана графа. А что, если мы попробуем построить эмбеддинги каким-нибудь другим способом? В этом задании мы предлагаем вам проявить немного фантазии. Возьмите какие-нибудь данные высокой размерности, чтобы задача обучения эмбеддингов имела смысл (например, картинки или тексты, желательно выбрать что-нибудь оригинальное). Придумайте или найдите какой-нибудь метод обучения эмбеддингов, примените его к данным и кластеризуйте полученные представления. Если чувствуете в себе достаточно силы, можете попробовать что-нибудь нейросетевое. Сравните ваш подход с базовыми алгоритмами кластеризации, которое мы рассмотрели в основном задании, не забывайте про визуализации! Ключевые слова для вдохновения: ***KernelPCA***, ***UMAP***, ***autoencoders***, ***gensim***."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sb68ky4oKgkh"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE ‿︵‿︵ヽ(°□° )ノ︵‿︵‿"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pc2cw6ZA1KFW"
      },
      "source": [
        "**Задание 7 (1 балл)**. Наконец, ставший ежегодной традицией социализационный бонус. Мы поощряем не только предметное, но и духовное развитие. Поэтому, чтобы заработать балл за это задание, сходите на какую-нибудь выставку, в музей, театр, или наконец в церковь, напишите небольшой отчетик о ваших впечатлениях и добавьте фотопруфы в ноутбук при сдаче. Можете объединиться с одногруппниками/однокурсниками, а также пригласить ассистентов/преподавателей, они тоже будут рады выбраться куда-нибудь. Для вдохновения приведем ссылку на актуальные выставки [новой](https://www.youtube.com/watch?v=dQw4w9WgXcQ&ab) и [старой Третьяковки](https://www.youtube.com/watch?v=xm3YgoEiEDc) (но совсем не обязательно посещать именно их)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "outk7jj91PuD"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE (ノಠ益ಠ)ノ彡┻━┻"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
